{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using Python-based geo functions\n",
      "Warning: RTree could not be loaded. areafilter get_intersecting and get_knearest won't work\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\npere\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\npere\\bluesky\\cache\\navdata.p\n",
      "Successfully loaded OpenAP performance model\n",
      "Failed to load BADA performance model\n",
      "Successfully loaded legacy performance model\n",
      "Successfully loaded plugin AREA\n",
      "Successfully loaded plugin DATAFEED\n"
     ]
    }
   ],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "# env = DescentEnv(render_mode='human')\n",
    "env = DescentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('altitude': Box(-inf, inf, (1,), float64), 'runway_distance': Box(-inf, inf, (1,), float64), 'target_altitude': Box(-inf, inf, (1,), float64), 'vz': Box(-inf, inf, (1,), float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.05263158, 0.10526316, 0.15789474, 0.21052632,\n",
       "       0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421,\n",
       "       0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211,\n",
       "       0.78947368, 0.84210526, 0.89473684, 0.94736842, 1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace(0, 1, 20)\n",
    "vertical_velocity_space = np.linspace(-10, 10, 20) \n",
    "target_altitude_space = np.linspace(0, 1, 20)\n",
    "runway_distance_space = np.linspace(0, 0.5, 10)\n",
    "altitude_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = np.clip(np.digitize(alt, altitude_space) - 1, 0, len(altitude_space) - 1)\n",
    "    vz_idx = np.clip(np.digitize(vz, vertical_velocity_space) - 1, 0, len(vertical_velocity_space) - 1)\n",
    "    target_alt_idx = np.clip(np.digitize(target_alt, target_altitude_space) - 1, 0, len(target_altitude_space) - 1)\n",
    "    runway_dist_idx = np.clip(np.digitize(runway_dist, runway_distance_space) - 1, 0, len(runway_distance_space) - 1)\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('altitude', array([1.10446054])), ('runway_distance', array([-0.00035892])), ('target_altitude', array([-0.72938554])), ('vz', array([0.43212574]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 9, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.7777777777777778,\n",
       " -0.5555555555555556,\n",
       " -0.33333333333333337,\n",
       " -0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.33333333333333326,\n",
       " 0.5555555555555554,\n",
       " 0.7777777777777777,\n",
       " 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(np.linspace(-1, 1, 10))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 10, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'altitude': array([0.66566667]), 'vz': array([0.]), 'target_altitude': array([0.771]), 'runway_distance': array([0.5])}\n",
      "total_reward -1.07644\n",
      "steps 1\n",
      "min_runway_distance: 0.4751241585463991\n",
      "max_runway_distance: 0.4751241585463991\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "\n",
    "min_runway_distance = float('inf')\n",
    "max_runway_distance = float('-inf')\n",
    "\n",
    "for _ in range(1):\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    action_idx = actions.index(action)\n",
    "    real_action = np.array([action])\n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "    # Guardar min y max runway_distance\n",
    "    runway_distance = obs['runway_distance'][0]\n",
    "    if runway_distance < min_runway_distance:\n",
    "        min_runway_distance = runway_distance\n",
    "    if runway_distance > max_runway_distance:\n",
    "        max_runway_distance = runway_distance\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        done = False\n",
    "\n",
    "env.close()\n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)\n",
    "print('min_runway_distance:', min_runway_distance)\n",
    "print('max_runway_distance:', max_runway_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# total_reward = 0\n",
    "# rewards = []\n",
    "# max_steps = 1\n",
    "\n",
    "# obs, _ = env.reset()\n",
    "# done = False\n",
    "\n",
    "# def get_explore_prob(i):\n",
    "#     # Entre 0 y 1000: explorar 90%\n",
    "#     # Entre 1001 y 2000: explotar 80%\n",
    "#     # Entre 2001 y 3000: explorar 90%\n",
    "#     # Entre 3001 y 4000: explotar 80%\n",
    "#     if 0 <= i <= 500:\n",
    "#         return 0.7\n",
    "#     elif 501 <= i <= 1000:\n",
    "#         return 0.3\n",
    "#     elif 1001 <= i <= 1500:\n",
    "#         return 0.7\n",
    "#     elif 1501 <= i <= 2000:\n",
    "#         return 0.3\n",
    "#     elif 2001 <= i <= 2500:\n",
    "#         return 0.7\n",
    "#     elif 2501 <= i <= 3000:\n",
    "#         return 0.3\n",
    "#     elif 3001 <= i <= 3500:\n",
    "#         return 0.7\n",
    "#     elif 3501 <= i <= 4000:\n",
    "#         return 0.3\n",
    "#     elif 4001 <= i <= 4500:\n",
    "#         return 0.7\n",
    "#     elif 4501 <= i <= 5000:\n",
    "#         return 0.3\n",
    "\n",
    "# while i < 5000:\n",
    "#     obs, _ = env.reset()\n",
    "#     done = False\n",
    "#     episode_reward = 0\n",
    "#     while not done:\n",
    "#         p = random.uniform(0, 1)\n",
    "#         state = get_state(obs)\n",
    "#         prob = get_explore_prob(i)\n",
    "#         if p < prob:\n",
    "#             action = get_sample_action()\n",
    "#         else:\n",
    "#             action = optimal_policy(state, Q)\n",
    "#         next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "#         next_state = get_state(next_obs)\n",
    "#         action_idx = actions.index(action)\n",
    "#         Q[state][action_idx] = Q[state][action_idx] + 0.9 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "#         obs = next_obs\n",
    "#         episode_reward += reward\n",
    "#     rewards.append(episode_reward)\n",
    "#     if (i + 1) % 100 == 0:\n",
    "#         mean_reward = np.mean(rewards[-100:])\n",
    "#         print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DescentEnv(render_mode='human')     \n",
    "# obs, _ = env.reset()\n",
    "# done = False\n",
    "# total_reward = 0\n",
    "# steps = 0\n",
    "\n",
    "# while not done:\n",
    "#     state = get_state(obs)\n",
    "#     action = optimal_policy(state, Q)\n",
    "#     obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "#     total_reward += reward\n",
    "#     steps += 1\n",
    "#     env.render()\n",
    "\n",
    "# env.close()\n",
    "# print(f\"Total reward (Q final): {total_reward}\")\n",
    "# print(f\"Steps: {steps}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def stoch_argmax(Q_values, k=None):\n",
    "    n = len(Q_values)\n",
    "    if k is None:\n",
    "        k = max(1, int(math.log2(n)))  # O(log(n))\n",
    "    subset = random.sample(range(n), k)\n",
    "    best_action = subset[0]\n",
    "    best_value = Q_values[best_action]\n",
    "    for i in subset[1:]:\n",
    "        if Q_values[i] > best_value:\n",
    "            best_action = i\n",
    "            best_value = Q_values[i]\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_stochmax = 3  \n",
    "rewards = []\n",
    "i=0\n",
    "def get_explore_prob(i):\n",
    "    \"\"\"\n",
    "    Devuelve la probabilidad de exploración (epsilon) según el número de episodio i.\n",
    "    Alterna entre 0.7 y 0.3 cada 500 episodios.\n",
    "    \"\"\"\n",
    "    if 0 <= i <= 500:\n",
    "        return 0.7\n",
    "    elif 501 <= i <= 1000:\n",
    "        return 0.3\n",
    "    elif 1001 <= i <= 1500:\n",
    "        return 0.7\n",
    "    elif 1501 <= i <= 2000:\n",
    "        return 0.3\n",
    "    elif 2001 <= i <= 2500:\n",
    "        return 0.7\n",
    "    elif 2501 <= i <= 3000:\n",
    "        return 0.3\n",
    "    elif 3001 <= i <= 3500:\n",
    "        return 0.7\n",
    "    elif 3501 <= i <= 4000:\n",
    "        return 0.3\n",
    "    elif 4001 <= i <= 4500:\n",
    "        return 0.7\n",
    "    elif 4501 <= i <= 5000:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.1  # valor por defecto fuera de rango\n",
    "\n",
    "while i < 5000:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action_idx = stoch_argmax(Q[state], k=k_stochmax)\n",
    "            action = actions[action_idx]\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        best_next_action_idx = stoch_argmax(Q[next_state], k=k_stochmax)\n",
    "        Q[state][action_idx] += 0.9 * (reward + 0.9 * Q[next_state][best_next_action_idx] - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
