{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\npere\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\npere\\bluesky\\cache\\navdata.p\n",
      "Successfully loaded OpenAP performance model\n",
      "Failed to load BADA performance model\n",
      "Successfully loaded legacy performance model\n",
      "Successfully loaded plugin AREA\n",
      "Successfully loaded plugin DATAFEED\n"
     ]
    }
   ],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "env = DescentEnv(render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('altitude': Box(-inf, inf, (1,), float64), 'runway_distance': Box(-inf, inf, (1,), float64), 'target_altitude': Box(-inf, inf, (1,), float64), 'vz': Box(-inf, inf, (1,), float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs['runway_distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16666667, 0.17340067, 0.18013468, 0.18686869, 0.19360269,\n",
       "       0.2003367 , 0.20707071, 0.21380471, 0.22053872, 0.22727273,\n",
       "       0.23400673, 0.24074074, 0.24747475, 0.25420875, 0.26094276,\n",
       "       0.26767677, 0.27441077, 0.28114478, 0.28787879, 0.29461279,\n",
       "       0.3013468 , 0.30808081, 0.31481481, 0.32154882, 0.32828283,\n",
       "       0.33501684, 0.34175084, 0.34848485, 0.35521886, 0.36195286,\n",
       "       0.36868687, 0.37542088, 0.38215488, 0.38888889, 0.3956229 ,\n",
       "       0.4023569 , 0.40909091, 0.41582492, 0.42255892, 0.42929293,\n",
       "       0.43602694, 0.44276094, 0.44949495, 0.45622896, 0.46296296,\n",
       "       0.46969697, 0.47643098, 0.48316498, 0.48989899, 0.496633  ,\n",
       "       0.503367  , 0.51010101, 0.51683502, 0.52356902, 0.53030303,\n",
       "       0.53703704, 0.54377104, 0.55050505, 0.55723906, 0.56397306,\n",
       "       0.57070707, 0.57744108, 0.58417508, 0.59090909, 0.5976431 ,\n",
       "       0.6043771 , 0.61111111, 0.61784512, 0.62457912, 0.63131313,\n",
       "       0.63804714, 0.64478114, 0.65151515, 0.65824916, 0.66498316,\n",
       "       0.67171717, 0.67845118, 0.68518519, 0.69191919, 0.6986532 ,\n",
       "       0.70538721, 0.71212121, 0.71885522, 0.72558923, 0.73232323,\n",
       "       0.73905724, 0.74579125, 0.75252525, 0.75925926, 0.76599327,\n",
       "       0.77272727, 0.77946128, 0.78619529, 0.79292929, 0.7996633 ,\n",
       "       0.80639731, 0.81313131, 0.81986532, 0.82659933, 0.83333333])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace((ALT_MIN - ALT_MEAN)/ALT_STD, (ALT_MAX - ALT_MEAN)/ALT_STD, 100)\n",
    "vertical_velocity_space = np.linspace(-10, 10, 100) \n",
    "target_altitude_space = np.linspace((ALT_MIN - ALT_MEAN)/ALT_STD, (ALT_MAX - ALT_MEAN)/ALT_STD, 100)\n",
    "runway_distance_space = np.linspace(-2, 2, 100)\n",
    "altitude_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = np.digitize(alt, altitude_space) - 1\n",
    "    vz_idx = np.digitize(vz, vertical_velocity_space) - 1\n",
    "    target_alt_idx = np.digitize(target_alt, target_altitude_space) - 1\n",
    "    runway_dist_idx = np.digitize(runway_dist, runway_distance_space) - 1\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('altitude', array([0.22241416])), ('runway_distance', array([1.65613662])), ('target_altitude', array([-0.03552804])), ('vz', array([-2.21991442]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 39, 0, 91)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.7777777777777778,\n",
       " -0.5555555555555556,\n",
       " -0.33333333333333337,\n",
       " -0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.33333333333333326,\n",
       " 0.5555555555555554,\n",
       " 0.7777777777777777,\n",
       " 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(np.linspace(-1, 1, 10))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100, 100, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,_ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "while not done:\n",
    "    steps += 1\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    \n",
    "    # Indice de la accion en Q\n",
    "    action_idx = actions.index(action)\n",
    "    \n",
    "    # Acción del ambiente\n",
    "    real_action = np.array([action])\n",
    "     \n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "   # Usar action_idx para actualizar Q\n",
    "   # Q[state][action_idx] = ... # Completar\n",
    "   \n",
    "   # Actualizar estado\n",
    "    state = next_state\n",
    "   \n",
    "    total_reward += reward\n",
    "\n",
    "    env.render()\n",
    "\n",
    "env.close()    \n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Mean Reward (last 10 steps): -2.9774563533333334\n",
      "Step 20: Mean Reward (last 10 steps): -13.472223342499998\n",
      "Step 30: Mean Reward (last 10 steps): -1.1708529288333334\n",
      "Step 40: Mean Reward (last 10 steps): -1.4093149175000002\n",
      "Step 50: Mean Reward (last 10 steps): -11.811536489000002\n",
      "Step 60: Mean Reward (last 10 steps): -0.5766127571666668\n",
      "Step 70: Mean Reward (last 10 steps): -12.017627941499999\n",
      "Step 80: Mean Reward (last 10 steps): -13.598227178000002\n",
      "Step 90: Mean Reward (last 10 steps): -2.3394006890000005\n",
      "Step 100: Mean Reward (last 10 steps): -12.499439333166668\n",
      "Total Reward after 100 steps: -718.7269193000004\n",
      "Training complete.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_reward = 0\n",
    "step_reward = 0\n",
    "step_count = 0\n",
    "max_steps = 1000\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while step_count < max_steps:\n",
    "    p = random.uniform(0, 1)\n",
    "    # Aumenta la probabilidad de exploración al inicio y la reduce con los pasos\n",
    "    exploration_threshold = max(0.1, 1.0 - (step_count / max_steps))\n",
    "    state = get_state(obs)\n",
    "    if p < exploration_threshold:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "    next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "    next_state = get_state(next_obs)\n",
    "    action_idx = actions.index(action)\n",
    "    Q[state][action_idx] = Q[state][action_idx] + 0.9 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "    obs = next_obs\n",
    "    total_reward += reward\n",
    "    step_reward += reward\n",
    "    step_count += 1\n",
    "    if step_count % 10 == 0:\n",
    "        mean_reward = step_reward / 10\n",
    "        print(f\"Step {step_count}: Mean Reward (last 10 steps): {mean_reward}\")\n",
    "        step_reward = 0\n",
    "    if step_count % 100 == 0:\n",
    "        env.render()\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "print(f\"Total Reward after {max_steps} steps: {total_reward}\")\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
