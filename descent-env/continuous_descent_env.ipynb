{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using Python-based geo functions\n",
      "Warning: RTree could not be loaded. areafilter get_intersecting and get_knearest won't work\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\Nico\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\Nico\\bluesky\\cache\\navdata.p\n",
      "Successfully loaded OpenAP performance model\n",
      "Failed to load BADA performance model\n",
      "Successfully loaded legacy performance model\n",
      "Successfully loaded plugin AREA\n",
      "Successfully loaded plugin DATAFEED\n"
     ]
    }
   ],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "# env = DescentEnv(render_mode='human')\n",
    "env = DescentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('altitude': Box(-inf, inf, (1,), float64), 'runway_distance': Box(-inf, inf, (1,), float64), 'target_altitude': Box(-inf, inf, (1,), float64), 'vz': Box(-inf, inf, (1,), float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.01449275, 0.02898551, 0.04347826, 0.05797101,\n",
       "       0.07246377, 0.08695652, 0.10144928, 0.11594203, 0.13043478,\n",
       "       0.14492754, 0.15942029, 0.17391304, 0.1884058 , 0.20289855,\n",
       "       0.2173913 , 0.23188406, 0.24637681, 0.26086957, 0.27536232,\n",
       "       0.28985507, 0.30434783, 0.31884058, 0.33333333, 0.34782609,\n",
       "       0.36231884, 0.37681159, 0.39130435, 0.4057971 , 0.42028986,\n",
       "       0.43478261, 0.44927536, 0.46376812, 0.47826087, 0.49275362,\n",
       "       0.50724638, 0.52173913, 0.53623188, 0.55072464, 0.56521739,\n",
       "       0.57971014, 0.5942029 , 0.60869565, 0.62318841, 0.63768116,\n",
       "       0.65217391, 0.66666667, 0.68115942, 0.69565217, 0.71014493,\n",
       "       0.72463768, 0.73913043, 0.75362319, 0.76811594, 0.7826087 ,\n",
       "       0.79710145, 0.8115942 , 0.82608696, 0.84057971, 0.85507246,\n",
       "       0.86956522, 0.88405797, 0.89855072, 0.91304348, 0.92753623,\n",
       "       0.94202899, 0.95652174, 0.97101449, 0.98550725, 1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace(0, 1, 70)\n",
    "vertical_velocity_space = np.linspace(-10, 10, 70) \n",
    "target_altitude_space = np.linspace(0, 1, 70)\n",
    "runway_distance_space = np.linspace(0, 0.5, 70)\n",
    "altitude_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = np.clip(np.digitize(alt, altitude_space) - 1, 0, len(altitude_space) - 1)\n",
    "    vz_idx = np.clip(np.digitize(vz, vertical_velocity_space) - 1, 0, len(vertical_velocity_space) - 1)\n",
    "    target_alt_idx = np.clip(np.digitize(target_alt, target_altitude_space) - 1, 0, len(target_altitude_space) - 1)\n",
    "    runway_dist_idx = np.clip(np.digitize(runway_dist, runway_distance_space) - 1, 0, len(runway_distance_space) - 1)\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.9310344827586207,\n",
       " -0.8620689655172413,\n",
       " -0.7931034482758621,\n",
       " -0.7241379310344828,\n",
       " -0.6551724137931034,\n",
       " -0.5862068965517242,\n",
       " -0.5172413793103449,\n",
       " -0.4482758620689655,\n",
       " -0.3793103448275862,\n",
       " -0.31034482758620685,\n",
       " -0.24137931034482762,\n",
       " -0.1724137931034483,\n",
       " -0.10344827586206895,\n",
       " -0.034482758620689724,\n",
       " 0.034482758620689724,\n",
       " 0.10344827586206895,\n",
       " 0.17241379310344818,\n",
       " 0.24137931034482762,\n",
       " 0.31034482758620685,\n",
       " 0.3793103448275863,\n",
       " 0.4482758620689655,\n",
       " 0.5172413793103448,\n",
       " 0.5862068965517242,\n",
       " 0.6551724137931034,\n",
       " 0.7241379310344827,\n",
       " 0.7931034482758621,\n",
       " 0.8620689655172413,\n",
       " 0.9310344827586206,\n",
       " 1.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(np.linspace(-1, 1, 30))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 70, 70, 70, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'altitude': array([0.44933333]), 'vz': array([0.]), 'target_altitude': array([0.351]), 'runway_distance': array([0.5])}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epsilon_greedy_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m max_runway_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Acción del modelo\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_greedy_policy\u001b[49m(state, Q, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     14\u001b[0m     action_idx \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mindex(action)\n\u001b[0;32m     15\u001b[0m     real_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([action])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epsilon_greedy_policy' is not defined"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "\n",
    "min_runway_distance = float('inf')\n",
    "max_runway_distance = float('-inf')\n",
    "\n",
    "for _ in range(1):\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    action_idx = actions.index(action)\n",
    "    real_action = np.array([action])\n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "    # Guardar min y max runway_distance\n",
    "    runway_distance = obs['runway_distance'][0]\n",
    "    if runway_distance < min_runway_distance:\n",
    "        min_runway_distance = runway_distance\n",
    "    if runway_distance > max_runway_distance:\n",
    "        max_runway_distance = runway_distance\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        done = False\n",
    "\n",
    "env.close()\n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)\n",
    "print('min_runway_distance:', min_runway_distance)\n",
    "print('max_runway_distance:', max_runway_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m optimal_policy(state, Q)\n\u001b[1;32m---> 32\u001b[0m next_obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m next_state \u001b[38;5;241m=\u001b[39m get_state(next_obs)\n\u001b[0;32m     34\u001b[0m action_idx \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mindex(action)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\descent_env.py:185\u001b[0m, in \u001b[0;36mDescentEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    183\u001b[0m action_frequency \u001b[38;5;241m=\u001b[39m ACTION_FREQUENCY\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_frequency):\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame()\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\simulation\\simulation.py:107\u001b[0m, in \u001b[0;36mSimulation.step\u001b[1;34m(self, dt_increment)\u001b[0m\n\u001b[0;32m    105\u001b[0m plotter\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    106\u001b[0m datalog\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 107\u001b[0m \u001b[43mhooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Determine interval towards next timestep                \u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimdt \u001b[38;5;241m=\u001b[39m simtime\u001b[38;5;241m.\u001b[39mstep(dt_increment)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\timedfunction.py:13\u001b[0m, in \u001b[0;36m_Hook.trigger\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrigger\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 13\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\timedfunction.py:51\u001b[0m, in \u001b[0;36mtimed_function.<locals>.deco.<locals>.callback\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fobj)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcallback\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timer\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mfobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_act\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\funcobject.py:32\u001b[0m, in \u001b[0;36mFuncObject.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\performance\\openap\\perfoap.py:219\u001b[0m, in \u001b[0;36mOpenAP.update\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrag[idx_fixwing] \u001b[38;5;241m=\u001b[39m rhovs \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcd0[idx_fixwing] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk[idx_fixwing] \u001b[38;5;241m*\u001b[39m cl \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# ----- compute maximum thrust -----\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m max_thrustratio_fixwing \u001b[38;5;241m=\u001b[39m \u001b[43mthrust\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_max_thr_ratio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengbpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtas\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malt\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengnum\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengthrmax\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_fixwing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_thrust[idx_fixwing] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    228\u001b[0m     max_thrustratio_fixwing\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengnum[idx_fixwing]\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengthrmax[idx_fixwing]\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# ----- compute net thrust -----\u001b[39;00m\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\performance\\openap\\thrust.py:6\u001b[0m, in \u001b[0;36mcompute_max_thr_ratio\u001b[1;34m(phase, bpr, v, h, vs, thr0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbluesky\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aero\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbluesky\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraffic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mperformance\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m phase \u001b[38;5;28;01mas\u001b[39;00m ph\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_max_thr_ratio\u001b[39m(phase, bpr, v, h, vs, thr0):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computer the dynamic thrust based on engine bypass-ratio, static maximum\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    thrust, aircraft true airspeed, and aircraft altitude\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m        int or 1D-array: thust in N\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(phase)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.stdout = open('output.txt', 'w', buffering=1)\n",
    "\n",
    "i = 0\n",
    "total_reward = 0\n",
    "rewards = []\n",
    "max_steps = 1\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "def get_explore_prob(i):\n",
    "    initial_epsilon = 0.9\n",
    "    min_epsilon = 0.05\n",
    "    decay_steps = 3530  # Redondeado hacia arriba\n",
    "    epsilon = max(initial_epsilon - (i // decay_steps) * 0.1, min_epsilon)\n",
    "    return epsilon\n",
    "\n",
    "while True:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action = optimal_policy(state, Q)\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        action_idx = actions.index(action)\n",
    "        Q[state][action_idx] = Q[state][action_idx] + 0.9 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    print(f\"Episode {i+1}, Reward: {episode_reward}, Epsilon: {prob}\")\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        with open('Q.pkl', 'wb') as f:\n",
    "            pickle.dump(Q, f)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Supongamos que Q es tu diccionario o matriz Q\n",
    "# Q = {...}\n",
    "\n",
    "# Guardar Q en un archivo .pkl\n",
    "with open('Q.pkl', 'wb') as f:\n",
    "    pickle.dump(Q, f)\n",
    "    \n",
    "import pprint\n",
    "\n",
    "pprint.pprint(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DescentEnv(render_mode='human')     \n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    state = get_state(obs)\n",
    "    action = optimal_policy(state, Q)\n",
    "    obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward (Q final): {total_reward}\")\n",
    "print(f\"Steps: {steps}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def stoch_argmax(Q_values, k=None):\n",
    "    n = len(Q_values)\n",
    "    if k is None:\n",
    "        k = max(1, int(math.log2(n)))  # O(log(n))\n",
    "    subset = random.sample(range(n), k)\n",
    "    best_action = subset[0]\n",
    "    best_value = Q_values[best_action]\n",
    "    for i in subset[1:]:\n",
    "        if Q_values[i] > best_value:\n",
    "            best_action = i\n",
    "            best_value = Q_values[i]\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_stochmax = 3  \n",
    "rewards = []\n",
    "i=0\n",
    "def get_explore_prob(i):\n",
    "    \"\"\"\n",
    "    Devuelve la probabilidad de exploración (epsilon) según el número de episodio i.\n",
    "    Alterna entre 0.7 y 0.3 cada 500 episodios.\n",
    "    \"\"\"\n",
    "    if 0 <= i <= 500:\n",
    "        return 0.7\n",
    "    elif 501 <= i <= 1000:\n",
    "        return 0.3\n",
    "    elif 1001 <= i <= 1500:\n",
    "        return 0.7\n",
    "    elif 1501 <= i <= 2000:\n",
    "        return 0.3\n",
    "    elif 2001 <= i <= 2500:\n",
    "        return 0.7\n",
    "    elif 2501 <= i <= 3000:\n",
    "        return 0.3\n",
    "    elif 3001 <= i <= 3500:\n",
    "        return 0.7\n",
    "    elif 3501 <= i <= 4000:\n",
    "        return 0.3\n",
    "    elif 4001 <= i <= 4500:\n",
    "        return 0.7\n",
    "    elif 4501 <= i <= 5000:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.1  # valor por defecto fuera de rango\n",
    "\n",
    "while i < 5000:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action_idx = stoch_argmax(Q[state], k=k_stochmax)\n",
    "            action = actions[action_idx]\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        best_next_action_idx = stoch_argmax(Q[next_state], k=k_stochmax)\n",
    "        Q[state][action_idx] += 0.9 * (reward + 0.9 * Q[next_state][best_next_action_idx] - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
