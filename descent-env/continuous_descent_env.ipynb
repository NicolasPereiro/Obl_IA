{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using Python-based geo functions\n",
      "Warning: RTree could not be loaded. areafilter get_intersecting and get_knearest won't work\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\npere\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\npere\\bluesky\\cache\\navdata.p\n",
      "Successfully loaded OpenAP performance model\n",
      "Failed to load BADA performance model\n",
      "Successfully loaded legacy performance model\n",
      "Successfully loaded plugin AREA\n",
      "Successfully loaded plugin DATAFEED\n"
     ]
    }
   ],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "# env = DescentEnv(render_mode='human')\n",
    "env = DescentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('altitude': Box(-inf, inf, (1,), float64), 'runway_distance': Box(-inf, inf, (1,), float64), 'target_altitude': Box(-inf, inf, (1,), float64), 'vz': Box(-inf, inf, (1,), float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace(0, 1, 20)           \n",
    "vertical_velocity_space = np.linspace(-5, 5, 15)  \n",
    "target_altitude_space = np.linspace(0, 1, 20)   \n",
    "runway_distance_space = np.linspace(0, 0.5, 15)  # 20 bins para distancia a la pista normalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = min(np.digitize(alt, altitude_space), len(altitude_space)-1)\n",
    "    vz_idx = min(np.digitize(vz, vertical_velocity_space), len(vertical_velocity_space)-1)\n",
    "    target_alt_idx = min(np.digitize(target_alt, target_altitude_space), len(target_altitude_space)-1)\n",
    "    runway_dist_idx = min(np.digitize(runway_dist, runway_distance_space), len(runway_distance_space)-1)\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('altitude', array([0.66164064])), ('runway_distance', array([0.10889728])), ('target_altitude', array([-1.95279903])), ('vz', array([0.51672287]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45, 36, 0, 15)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.8571428571428572,\n",
       " -0.7142857142857143,\n",
       " -0.5714285714285714,\n",
       " -0.4285714285714286,\n",
       " -0.2857142857142858,\n",
       " -0.1428571428571429,\n",
       " 0.0,\n",
       " 0.1428571428571428,\n",
       " 0.2857142857142856,\n",
       " 0.4285714285714284,\n",
       " 0.5714285714285714,\n",
       " 0.7142857142857142,\n",
       " 0.857142857142857,\n",
       " 1.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(np.linspace(-1, 1, 15))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 15, 20, 15, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "\n",
    "min_runway_distance = float('inf')\n",
    "max_runway_distance = float('-inf')\n",
    "\n",
    "for _ in range(1):\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    action_idx = actions.index(action)\n",
    "    real_action = np.array([action])\n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "    # Guardar min y max runway_distance\n",
    "    runway_distance = obs['runway_distance'][0]\n",
    "    if runway_distance < min_runway_distance:\n",
    "        min_runway_distance = runway_distance\n",
    "    if runway_distance > max_runway_distance:\n",
    "        max_runway_distance = runway_distance\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        done = False\n",
    "\n",
    "env.close()\n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)\n",
    "print('min_runway_distance:', min_runway_distance)\n",
    "print('max_runway_distance:', max_runway_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.stdout = open('output.txt', 'w', buffering=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     action \u001b[38;5;241m=\u001b[39m optimal_policy(state, Q)\n\u001b[1;32m---> 22\u001b[0m next_obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m next_state \u001b[38;5;241m=\u001b[39m get_state(next_obs)\n\u001b[0;32m     24\u001b[0m action_idx \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mindex(action)\n",
      "File \u001b[1;32mc:\\ID\\SEM7\\Obl_IA\\descent-env\\descent_env.py:186\u001b[0m, in \u001b[0;36mDescentEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    184\u001b[0m action_frequency \u001b[38;5;241m=\u001b[39m ACTION_FREQUENCY\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_frequency):\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame()\n",
      "File \u001b[1;32mc:\\ID\\SEM7\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\simulation\\simulation.py:116\u001b[0m, in \u001b[0;36mSimulation.step\u001b[1;34m(self, dt_increment)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimdt)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Update traffic and other update functions for the next timestep\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     hooks\u001b[38;5;241m.\u001b[39mupdate\u001b[38;5;241m.\u001b[39mtrigger()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ID\\SEM7\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\traffic.py:403\u001b[0m, in \u001b[0;36mTraffic.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madsb\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m#---------- Fly the Aircraft --------------------------\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Autopilot logic\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Conflict detection and resolution\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masastimer\u001b[38;5;241m.\u001b[39mreadynext():\n",
      "File \u001b[1;32mc:\\ID\\SEM7\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\autopilot.py:309\u001b[0m, in \u001b[0;36mAutopilot.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# FMS LNAV mode:\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;66;03m# qdr[deg],distinnm[nm]\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     qdr, distinnm \u001b[38;5;241m=\u001b[39m \u001b[43mgeo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqdrdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactwp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactwp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlon\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [deg][nm])\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqdr2wp  \u001b[38;5;241m=\u001b[39m qdr\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist2wp \u001b[38;5;241m=\u001b[39m distinnm\u001b[38;5;241m*\u001b[39mnm  \u001b[38;5;66;03m# Conversion to meters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ID\\SEM7\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\tools\\geo\\_geo.py:93\u001b[0m, in \u001b[0;36mqdrdist\u001b[1;34m(latd1, lond1, latd2, lond2)\u001b[0m\n\u001b[0;32m     90\u001b[0m r    \u001b[38;5;241m=\u001b[39m sw \u001b[38;5;241m*\u001b[39m res1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sw) \u001b[38;5;241m*\u001b[39m res2\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Convert to radians\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m lat1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradians\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatd1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m lon1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mradians(lond1)\n\u001b[0;32m     95\u001b[0m lat2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mradians(latd2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 8000\n",
    "total_reward = 0\n",
    "rewards = []\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "def get_explore_prob(i):\n",
    "    return 0.2\n",
    "    \n",
    "while True:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action = optimal_policy(state, Q)\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        action_idx = actions.index(action)\n",
    "        Q[state][action_idx] = Q[state][action_idx] + 0.5 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploracion {prob}\")\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        with open('Q.pkl', 'wb') as f:\n",
    "            pickle.dump(Q, f)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo: <class 'numpy.ndarray'>\n",
      "Shape: (20, 15, 20, 15, 15)\n",
      "Tamaño en memoria (MB): 10.2996826171875\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Q.pkl', 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "\n",
    "print(\"Tipo:\", type(Q))\n",
    "print(\"Shape:\", Q.shape)\n",
    "print(\"Tamaño en memoria (MB):\", Q.nbytes / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación con 100 episodios usando la política aprendida (greedy)\n",
    "env = DescentEnv()\n",
    "total_rewards = []\n",
    "total_steps = []\n",
    "\n",
    "for episode in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        state = get_state(obs)\n",
    "        action = optimal_policy(state, Q)  # Política greedy\n",
    "        obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        # Si no quieres render, comenta la siguiente línea\n",
    "        # env.render()\n",
    "    total_rewards.append(episode_reward)\n",
    "    total_steps.append(steps)\n",
    "\n",
    "env.close()\n",
    "print(f\"Recompensa media en 100 episodios: {np.mean(total_rewards)}\")\n",
    "print(f\"Recompensa minima: {np.min(total_rewards)}\")\n",
    "print(f\"Recompensa maxima: {np.max(total_rewards)}\")\n",
    "print(f\"Pasos promedio por episodio: {np.mean(total_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabacion del resultado\n",
    "from env_recorder_wrapper import VideoRecorderWrapper\n",
    "env = DescentEnv(render_mode=\"rgb_array\")\n",
    "env = VideoRecorderWrapper(env, fps=10) \n",
    "\n",
    "\n",
    "total_rewards = []\n",
    "total_steps = []\n",
    "\n",
    "for episode in range(1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        state = get_state(obs)\n",
    "        action = optimal_policy(state, Q) \n",
    "        obs, reward, done, truncated, _ = env.step(np.array([action]))\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        if done or truncated:\n",
    "            break\n",
    "    total_rewards.append(episode_reward)\n",
    "    total_steps.append(steps)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DescentEnv(render_mode=\"rgb_array\")\n",
    "frame = env.render()\n",
    "print(type(frame), frame.shape if frame is not None else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def stoch_argmax(Q_values, k=None):\n",
    "    n = len(Q_values)\n",
    "    if k is None:\n",
    "        k = max(1, int(math.log2(n)))  # O(log(n))\n",
    "    subset = random.sample(range(n), k)\n",
    "    best_action = subset[0]\n",
    "    best_value = Q_values[best_action]\n",
    "    for i in subset[1:]:\n",
    "        if Q_values[i] > best_value:\n",
    "            best_action = i\n",
    "            best_value = Q_values[i]\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_stochmax = 3  \n",
    "rewards = []\n",
    "i=0\n",
    "def get_explore_prob(i):\n",
    "    \"\"\"\n",
    "    Devuelve la probabilidad de exploración (epsilon) según el número de episodio i.\n",
    "    Alterna entre 0.7 y 0.3 cada 500 episodios.\n",
    "    \"\"\"\n",
    "    if 0 <= i <= 500:\n",
    "        return 0.7\n",
    "    elif 501 <= i <= 1000:\n",
    "        return 0.3\n",
    "    elif 1001 <= i <= 1500:\n",
    "        return 0.7\n",
    "    elif 1501 <= i <= 2000:\n",
    "        return 0.3\n",
    "    elif 2001 <= i <= 2500:\n",
    "        return 0.7\n",
    "    elif 2501 <= i <= 3000:\n",
    "        return 0.3\n",
    "    elif 3001 <= i <= 3500:\n",
    "        return 0.7\n",
    "    elif 3501 <= i <= 4000:\n",
    "        return 0.3\n",
    "    elif 4001 <= i <= 4500:\n",
    "        return 0.7\n",
    "    elif 4501 <= i <= 5000:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.1  # valor por defecto fuera de rango\n",
    "\n",
    "while i < 5000:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action_idx = stoch_argmax(Q[state], k=k_stochmax)\n",
    "            action = actions[action_idx]\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        best_next_action_idx = stoch_argmax(Q[next_state], k=k_stochmax)\n",
    "        Q[state][action_idx] += 0.9 * (reward + 0.9 * Q[next_state][best_next_action_idx] - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
