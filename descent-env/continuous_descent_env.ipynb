{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using Python-based geo functions\n",
      "Warning: RTree could not be loaded. areafilter get_intersecting and get_knearest won't work\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\npere\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\npere\\bluesky\\cache\\navdata.p\n",
      "Successfully loaded OpenAP performance model\n",
      "Failed to load BADA performance model\n",
      "Successfully loaded legacy performance model\n",
      "Successfully loaded plugin AREA\n",
      "Successfully loaded plugin DATAFEED\n"
     ]
    }
   ],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "# env = DescentEnv(render_mode='human')\n",
    "env = DescentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('altitude': Box(-inf, inf, (1,), float64), 'runway_distance': Box(-inf, inf, (1,), float64), 'target_altitude': Box(-inf, inf, (1,), float64), 'vz': Box(-inf, inf, (1,), float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.05263158, 0.10526316, 0.15789474, 0.21052632,\n",
       "       0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421,\n",
       "       0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211,\n",
       "       0.78947368, 0.84210526, 0.89473684, 0.94736842, 1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace(0, 1, 20)\n",
    "vertical_velocity_space = np.linspace(-10, 10, 20) \n",
    "target_altitude_space = np.linspace(0, 1, 20)\n",
    "runway_distance_space = np.linspace(0, 0.5, 10)\n",
    "altitude_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = np.clip(np.digitize(alt, altitude_space) - 1, 0, len(altitude_space) - 1)\n",
    "    vz_idx = np.clip(np.digitize(vz, vertical_velocity_space) - 1, 0, len(vertical_velocity_space) - 1)\n",
    "    target_alt_idx = np.clip(np.digitize(target_alt, target_altitude_space) - 1, 0, len(target_altitude_space) - 1)\n",
    "    runway_dist_idx = np.clip(np.digitize(runway_dist, runway_distance_space) - 1, 0, len(runway_distance_space) - 1)\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('altitude', array([0.98839454])), ('runway_distance', array([-0.5512305])), ('target_altitude', array([1.95661496])), ('vz', array([0.18128256]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18, 9, 19, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.7777777777777778,\n",
       " -0.5555555555555556,\n",
       " -0.33333333333333337,\n",
       " -0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.33333333333333326,\n",
       " 0.5555555555555554,\n",
       " 0.7777777777777777,\n",
       " 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(np.linspace(-1, 1, 10))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 10, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'altitude': array([0.28333333]), 'vz': array([0.]), 'target_altitude': array([0.28766667]), 'runway_distance': array([0.5])}\n",
      "total_reward -0.5281066666666667\n",
      "steps 1\n",
      "min_runway_distance: 0.4747557822672158\n",
      "max_runway_distance: 0.4747557822672158\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "\n",
    "min_runway_distance = float('inf')\n",
    "max_runway_distance = float('-inf')\n",
    "\n",
    "for _ in range(1):\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    action_idx = actions.index(action)\n",
    "    real_action = np.array([action])\n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "    # Guardar min y max runway_distance\n",
    "    runway_distance = obs['runway_distance'][0]\n",
    "    if runway_distance < min_runway_distance:\n",
    "        min_runway_distance = runway_distance\n",
    "    if runway_distance > max_runway_distance:\n",
    "        max_runway_distance = runway_distance\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        done = False\n",
    "\n",
    "env.close()\n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)\n",
    "print('min_runway_distance:', min_runway_distance)\n",
    "print('max_runway_distance:', max_runway_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de reward en episodios 1 a 100: -140.51660472803334 con exploración 0.7\n",
      "Media de reward en episodios 101 a 200: -127.92853751565002 con exploración 0.7\n",
      "Media de reward en episodios 201 a 300: -119.98375159543333 con exploración 0.7\n",
      "Media de reward en episodios 301 a 400: -108.68940492473334 con exploración 0.7\n",
      "Media de reward en episodios 401 a 500: -107.22313709083335 con exploración 0.7\n",
      "Media de reward en episodios 501 a 600: -117.8310943839 con exploración 0.3\n",
      "Media de reward en episodios 601 a 700: -107.22835869780002 con exploración 0.3\n",
      "Media de reward en episodios 701 a 800: -107.66582846231668 con exploración 0.3\n",
      "Media de reward en episodios 801 a 900: -98.61195811773332 con exploración 0.3\n",
      "Media de reward en episodios 901 a 1000: -95.30076135106665 con exploración 0.3\n",
      "Media de reward en episodios 1001 a 1100: -95.8419418746 con exploración 0.7\n",
      "Media de reward en episodios 1101 a 1200: -97.34409445776666 con exploración 0.7\n",
      "Media de reward en episodios 1201 a 1300: -93.62665083139999 con exploración 0.7\n",
      "Media de reward en episodios 1301 a 1400: -90.02629765918333 con exploración 0.7\n",
      "Media de reward en episodios 1401 a 1500: -98.83450150703335 con exploración 0.7\n",
      "Media de reward en episodios 1501 a 1600: -92.79194741788335 con exploración 0.3\n",
      "Media de reward en episodios 1601 a 1700: -88.38919867733334 con exploración 0.3\n",
      "Media de reward en episodios 1701 a 1800: -84.69007764758334 con exploración 0.3\n",
      "Media de reward en episodios 1801 a 1900: -84.46474354135 con exploración 0.3\n",
      "Media de reward en episodios 1901 a 2000: -79.74545531906666 con exploración 0.3\n",
      "Media de reward en episodios 2001 a 2100: -83.21043866630002 con exploración 0.7\n",
      "Media de reward en episodios 2101 a 2200: -82.88741299966667 con exploración 0.7\n",
      "Media de reward en episodios 2201 a 2300: -88.01964764646665 con exploración 0.7\n",
      "Media de reward en episodios 2301 a 2400: -83.09171942651666 con exploración 0.7\n",
      "Media de reward en episodios 2401 a 2500: -91.25317206111667 con exploración 0.7\n",
      "Media de reward en episodios 2501 a 2600: -77.44551340956667 con exploración 0.3\n",
      "Media de reward en episodios 2601 a 2700: -74.30341821125 con exploración 0.3\n",
      "Media de reward en episodios 2701 a 2800: -72.20925490788333 con exploración 0.3\n",
      "Media de reward en episodios 2801 a 2900: -69.24780039043333 con exploración 0.3\n",
      "Media de reward en episodios 2901 a 3000: -68.61626184865 con exploración 0.3\n",
      "Media de reward en episodios 3001 a 3100: -87.21167469293334 con exploración 0.7\n",
      "Media de reward en episodios 3101 a 3200: -83.54173779081668 con exploración 0.7\n",
      "Media de reward en episodios 3201 a 3300: -83.8622284141 con exploración 0.7\n",
      "Media de reward en episodios 3301 a 3400: -79.28341579725 con exploración 0.7\n",
      "Media de reward en episodios 3401 a 3500: -78.34612405468333 con exploración 0.7\n",
      "Media de reward en episodios 3501 a 3600: -74.9200066667 con exploración 0.3\n",
      "Media de reward en episodios 3601 a 3700: -70.4617021471 con exploración 0.3\n",
      "Media de reward en episodios 3701 a 3800: -67.50868298126666 con exploración 0.3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_reward = 0\n",
    "rewards = []\n",
    "max_steps = 1\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "def get_explore_prob(i):\n",
    "    # Entre 0 y 1000: explorar 90%\n",
    "    # Entre 1001 y 2000: explotar 80%\n",
    "    # Entre 2001 y 3000: explorar 90%\n",
    "    # Entre 3001 y 4000: explotar 80%\n",
    "    if 0 <= i <= 500:\n",
    "        return 0.7\n",
    "    elif 501 <= i <= 1000:\n",
    "        return 0.3\n",
    "    elif 1001 <= i <= 1500:\n",
    "        return 0.7\n",
    "    elif 1501 <= i <= 2000:\n",
    "        return 0.3\n",
    "    elif 2001 <= i <= 2500:\n",
    "        return 0.7\n",
    "    elif 2501 <= i <= 3000:\n",
    "        return 0.3\n",
    "    elif 3001 <= i <= 3500:\n",
    "        return 0.7\n",
    "    elif 3501 <= i <= 4000:\n",
    "        return 0.3\n",
    "    elif 4001 <= i <= 4500:\n",
    "        return 0.7\n",
    "    elif 4501 <= i <= 5000:\n",
    "        return 0.3\n",
    "\n",
    "while i < 5000:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action = optimal_policy(state, Q)\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        action_idx = actions.index(action)\n",
    "        Q[state][action_idx] = Q[state][action_idx] + 0.9 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\npere\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\npere\\bluesky\\cache\\navdata.p\n",
      "Attempt to reimplement AREA from <bound method Area.set_area of <bluesky.plugins.area.Area object at 0x00000278435B1A50>> to <bound method Area.set_area of <bluesky.plugins.area.Area object at 0x00000278435B1A50>>\n",
      "Attempt to reimplement EXP from <function init_plugin.<locals>.<lambda> at 0x000002781D7D3EB0> to <function init_plugin.<locals>.<lambda> at 0x000002784355FE20>\n",
      "Attempt to reimplement TAXI from <bound method Area.set_taxi of <bluesky.plugins.area.Area object at 0x00000278435B1A50>> to <bound method Area.set_taxi of <bluesky.plugins.area.Area object at 0x00000278435B1A50>>\n",
      "Successfully loaded plugin AREA\n",
      "Attempt to reimplement DATAFEED from <bound method Modesbeast.toggle of <bluesky.plugins.adsbfeed.Modesbeast object at 0x00000278435B1AB0>> to <bound method Modesbeast.toggle of <bluesky.plugins.adsbfeed.Modesbeast object at 0x00000278351CBF70>>\n",
      "Successfully loaded plugin DATAFEED\n",
      "Attempt to reimplement PLUGINS from <function init.<locals>.manage at 0x00000278435B5090> to <function init.<locals>.manage at 0x0000027843525510>\n",
      "Attempt to reimplement ADDNODES from <bound method Node.addnodes of <bluesky.network.detached.Node object at 0x000002784350E200>> to <bound method Node.addnodes of <bluesky.network.detached.Node object at 0x000002783DECCF10>>\n",
      "Attempt to reimplement AIRWAY from <bound method Traffic.airwaycmd of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.airwaycmd of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement ATALT from <bound method Condition.ataltcmd of <bluesky.traffic.conditional.Condition object at 0x000002784350EC80>> to <bound method Condition.ataltcmd of <bluesky.traffic.conditional.Condition object at 0x000002784350EC80>>\n",
      "Attempt to reimplement ATDIST from <bound method Condition.atdistcmd of <bluesky.traffic.conditional.Condition object at 0x000002784350EC80>> to <bound method Condition.atdistcmd of <bluesky.traffic.conditional.Condition object at 0x000002784350EC80>>\n",
      "Attempt to reimplement ATSPD from <bound method Condition.atspdcmd of <bluesky.traffic.conditional.Condition object at 0x000002784350EC80>> to <bound method Condition.atspdcmd of <bluesky.traffic.conditional.Condition object at 0x000002784350EC80>>\n",
      "Attempt to reimplement BANK from <bound method Traffic.setbanklim of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.setbanklim of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement BATCH from <bound method Simulation.batch of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.batch of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement BLUESKY from <function singbluesky at 0x00000278351D48B0> to <function singbluesky at 0x00000278351D48B0>\n",
      "Attempt to reimplement BENCHMARK from <bound method Simulation.benchmark of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.benchmark of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement BOX from <function initbasecmds.<locals>.<lambda> at 0x000002784355D630> to <function initbasecmds.<locals>.<lambda> at 0x0000027843525510>\n",
      "Attempt to reimplement CALC from <function calculator at 0x00000278351D4820> to <function calculator at 0x00000278351D4820>\n",
      "Attempt to reimplement CASMACHTHR from <function casmachthr at 0x0000027835184B80> to <function casmachthr at 0x0000027835184B80>\n",
      "Attempt to reimplement CD from <function setscenpath at 0x00000278351D49D0> to <function setscenpath at 0x00000278351D49D0>\n",
      "Attempt to reimplement CIRCLE from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5120> to <function initbasecmds.<locals>.<lambda> at 0x00000278435244C0>\n",
      "Attempt to reimplement CLRCRECMD from <bound method Traffic.clrcrecmd of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.clrcrecmd of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement CRE from <bound method Traffic.cre of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.cre of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement CRECMD from <bound method Traffic.crecmd of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.crecmd of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement CRECONFS from <bound method Traffic.creconfs of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.creconfs of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement DATE from <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement DEFWPT from <bound method Navdatabase.defwpt of <bluesky.navdatabase.navdatabase.Navdatabase object at 0x00000278351CBFA0>> to <bound method Navdatabase.defwpt of <bluesky.navdatabase.navdatabase.Navdatabase object at 0x0000027834EF7130>>\n",
      "Attempt to reimplement DEL from <function initbasecmds.<locals>.<lambda> at 0x00000278435B55A0> to <function initbasecmds.<locals>.<lambda> at 0x0000027843525120>\n",
      "Attempt to reimplement DIST from <function distcalc at 0x00000278351D4940> to <function distcalc at 0x00000278351D4940>\n",
      "Attempt to reimplement DOC from <bound method ScreenIO.show_cmd_doc of <bluesky.simulation.screenio.ScreenIO object at 0x000002784350CFA0>> to <bound method ScreenIO.show_cmd_doc of <bluesky.simulation.screenio.ScreenIO object at 0x000002784350CFA0>>\n",
      "Attempt to reimplement DT from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5630> to <function initbasecmds.<locals>.<lambda> at 0x0000027843509990>\n",
      "Attempt to reimplement DTMULT from <bound method Simulation.set_dtmult of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.set_dtmult of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement ECHO from <function initbasecmds.<locals>.<lambda> at 0x00000278435B56C0> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B79A0>\n",
      "Attempt to reimplement FF from <bound method Simulation.fastforward of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.fastforward of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement FIXDT from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5750> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B6C20>\n",
      "Attempt to reimplement GROUP from <bound method TrafficGroups.group of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x00000278434EA8C0>> to <bound method TrafficGroups.group of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x00000278434EA8C0>>\n",
      "Attempt to reimplement HOLD from <bound method Simulation.hold of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.hold of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement IMPLEMENTATION from <function select_implementation at 0x00000278351291B0> to <function select_implementation at 0x00000278351291B0>\n",
      "Attempt to reimplement INSEDIT from <bound method ScreenIO.cmdline of <bluesky.simulation.screenio.ScreenIO object at 0x000002784350CFA0>> to <bound method ScreenIO.cmdline of <bluesky.simulation.screenio.ScreenIO object at 0x000002784350CFA0>>\n",
      "Attempt to reimplement LEGEND from <function initbasecmds.<locals>.<lambda> at 0x00000278435B57E0> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B7640>\n",
      "Attempt to reimplement LINE from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5870> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B71C0>\n",
      "Attempt to reimplement LSVAR from <function lsvar at 0x0000027835158CA0> to <function lsvar at 0x0000027835158CA0>\n",
      "Attempt to reimplement MAGVAR from <function magdeccmd at 0x0000027835186B00> to <function magdeccmd at 0x0000027835186B00>\n",
      "Attempt to reimplement MCRE from <bound method Traffic.mcre of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.mcre of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement MOVE from <bound method Traffic.move of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.move of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement NOISE from <bound method Traffic.setnoise of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.setnoise of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement OP from <bound method Simulation.op of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.op of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement PLOT from <function plot at 0x00000278351AF370> to <function plot at 0x00000278351AF370>\n",
      "Attempt to reimplement POLY from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5900> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B7490>\n",
      "Attempt to reimplement POLYALT from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5990> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B7250>\n",
      "Attempt to reimplement POLYLINE from <function initbasecmds.<locals>.<lambda> at 0x00000278435B5A20> to <function initbasecmds.<locals>.<lambda> at 0x00000278435B6EF0>\n",
      "Attempt to reimplement POS from <bound method Traffic.poscommand of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.poscommand of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement REALTIME from <bound method Simulation.realtime of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.realtime of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement RESET from <bound method Simulation.reset of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.reset of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement SEED from <function Simulation.setseed at 0x00000278351D6C20> to <function Simulation.setseed at 0x00000278351D6C20>\n",
      "Attempt to reimplement THR from <bound method Traffic.setthrottle of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>> to <bound method Traffic.setthrottle of <bluesky.traffic.traffic.Traffic object at 0x00000278351CBEB0>>\n",
      "Attempt to reimplement TIME from <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x00000278351CBEE0>> to <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x000002784350E890>>\n",
      "Attempt to reimplement TRAIL from <bound method Trails.setTrails of <bluesky.traffic.trails.Trails object at 0x000002783DECC7C0>> to <bound method Trails.setTrails of <bluesky.traffic.trails.Trails object at 0x000002783DECC7C0>>\n",
      "Attempt to reimplement UNGROUP from <bound method TrafficGroups.ungroup of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x00000278434EA8C0>> to <bound method TrafficGroups.ungroup of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x00000278434EA8C0>>\n",
      "Total reward (Q final): -66.40556405666668\n",
      "Steps: 41\n"
     ]
    }
   ],
   "source": [
    "env = DescentEnv(render_mode='human')     \n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    state = get_state(obs)\n",
    "    action = optimal_policy(state, Q)\n",
    "    obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward (Q final): {total_reward}\")\n",
    "print(f\"Steps: {steps}\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
