{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using Python-based geo functions\n",
      "Warning: RTree could not be loaded. areafilter get_intersecting and get_knearest won't work\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\agusp\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\agusp\\bluesky\\cache\\navdata.p\n",
      "Successfully loaded OpenAP performance model\n",
      "Failed to load BADA performance model\n",
      "Successfully loaded legacy performance model\n",
      "Successfully loaded plugin AREA\n",
      "Successfully loaded plugin DATAFEED\n"
     ]
    }
   ],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "# env = DescentEnv(render_mode='human')\n",
    "env = DescentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('altitude': Box(-inf, inf, (1,), float64), 'runway_distance': Box(-inf, inf, (1,), float64), 'target_altitude': Box(-inf, inf, (1,), float64), 'vz': Box(-inf, inf, (1,), float64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.03448276, 0.06896552, 0.10344828, 0.13793103,\n",
       "       0.17241379, 0.20689655, 0.24137931, 0.27586207, 0.31034483,\n",
       "       0.34482759, 0.37931034, 0.4137931 , 0.44827586, 0.48275862,\n",
       "       0.51724138, 0.55172414, 0.5862069 , 0.62068966, 0.65517241,\n",
       "       0.68965517, 0.72413793, 0.75862069, 0.79310345, 0.82758621,\n",
       "       0.86206897, 0.89655172, 0.93103448, 0.96551724, 1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace(0, 1, 30)\n",
    "vertical_velocity_space = np.linspace(-10, 10, 30) \n",
    "target_altitude_space = np.linspace(0, 1, 30)\n",
    "runway_distance_space = np.linspace(0, 0.5, 10)\n",
    "altitude_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = np.clip(np.digitize(alt, altitude_space) - 1, 0, len(altitude_space) - 1)\n",
    "    vz_idx = np.clip(np.digitize(vz, vertical_velocity_space) - 1, 0, len(vertical_velocity_space) - 1)\n",
    "    target_alt_idx = np.clip(np.digitize(target_alt, target_altitude_space) - 1, 0, len(target_altitude_space) - 1)\n",
    "    runway_dist_idx = np.clip(np.digitize(runway_dist, runway_distance_space) - 1, 0, len(runway_distance_space) - 1)\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('altitude', array([0.34711037])), ('runway_distance', array([0.99038471])), ('target_altitude', array([0.56185805])), ('vz', array([-1.23348683]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.7777777777777778,\n",
       " -0.5555555555555556,\n",
       " -0.33333333333333337,\n",
       " -0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.33333333333333326,\n",
       " 0.5555555555555554,\n",
       " 0.7777777777777777,\n",
       " 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(np.linspace(-1, 1, 10))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 30, 10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'altitude': array([0.28284088]), 'vz': array([0.27777778]), 'target_altitude': array([0.38833333]), 'runway_distance': array([0.37619696])}\n",
      "total_reward -82423.0027900281\n",
      "steps 10000\n",
      "min_runway_distance: -0.5212062905283475\n",
      "max_runway_distance: 0.47556561005149406\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "\n",
    "min_runway_distance = float('inf')\n",
    "max_runway_distance = float('-inf')\n",
    "\n",
    "for _ in range(10000):\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    action_idx = actions.index(action)\n",
    "    real_action = np.array([action])\n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "    # Guardar min y max runway_distance\n",
    "    runway_distance = obs['runway_distance'][0]\n",
    "    if runway_distance < min_runway_distance:\n",
    "        min_runway_distance = runway_distance\n",
    "    if runway_distance > max_runway_distance:\n",
    "        max_runway_distance = runway_distance\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        done = False\n",
    "\n",
    "env.close()\n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)\n",
    "print('min_runway_distance:', min_runway_distance)\n",
    "print('max_runway_distance:', max_runway_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1: reward = -89.16632848333334, exploración = 0.9\n",
      "Episodio 2: reward = -100.519939815, exploración = 0.9\n",
      "Episodio 3: reward = -203.70997538, exploración = 0.9\n",
      "Episodio 4: reward = -217.73698022000002, exploración = 0.9\n",
      "Episodio 5: reward = -87.87749048333335, exploración = 0.9\n",
      "Episodio 6: reward = -70.47574244666667, exploración = 0.9\n",
      "Episodio 7: reward = -103.54508782166667, exploración = 0.9\n",
      "Episodio 8: reward = -121.14217666666667, exploración = 0.9\n",
      "Episodio 9: reward = -79.280641, exploración = 0.9\n",
      "Episodio 10: reward = -62.34428961166667, exploración = 0.9\n",
      "Episodio 11: reward = -142.98870174166666, exploración = 0.9\n",
      "Episodio 12: reward = -75.49618119666667, exploración = 0.9\n",
      "Episodio 13: reward = -93.066142765, exploración = 0.9\n",
      "Episodio 14: reward = -66.78909018, exploración = 0.9\n",
      "Episodio 15: reward = -76.55805046166667, exploración = 0.9\n",
      "Episodio 16: reward = -83.22737474166667, exploración = 0.9\n",
      "Episodio 17: reward = -118.28935846499999, exploración = 0.9\n",
      "Episodio 18: reward = -133.12752875166666, exploración = 0.9\n",
      "Episodio 19: reward = -110.93937707333333, exploración = 0.9\n",
      "Episodio 20: reward = -76.58189577833332, exploración = 0.9\n",
      "Episodio 21: reward = -130.12989666833334, exploración = 0.9\n",
      "Episodio 22: reward = -167.7061694266667, exploración = 0.9\n",
      "Episodio 23: reward = -130.3416632366667, exploración = 0.9\n",
      "Episodio 24: reward = -57.005602718333336, exploración = 0.9\n",
      "Episodio 25: reward = -138.65000192666668, exploración = 0.9\n",
      "Episodio 26: reward = -127.51421672999999, exploración = 0.9\n",
      "Episodio 27: reward = -123.16889992833333, exploración = 0.9\n",
      "Episodio 28: reward = -84.44148634833333, exploración = 0.9\n",
      "Episodio 29: reward = -104.11955656166668, exploración = 0.9\n",
      "Episodio 30: reward = -97.91171754499999, exploración = 0.9\n",
      "Episodio 31: reward = -48.848554495, exploración = 0.9\n",
      "Episodio 32: reward = -106.329091645, exploración = 0.9\n",
      "Episodio 33: reward = -110.18524292666666, exploración = 0.9\n",
      "Episodio 34: reward = -95.30777495333332, exploración = 0.9\n",
      "Episodio 35: reward = -165.864834165, exploración = 0.9\n",
      "Episodio 36: reward = -55.45901065000001, exploración = 0.9\n",
      "Episodio 37: reward = -136.47349086666668, exploración = 0.9\n",
      "Episodio 38: reward = -171.51233937666666, exploración = 0.9\n",
      "Episodio 39: reward = -151.17044142500004, exploración = 0.9\n",
      "Episodio 40: reward = -211.77121943499998, exploración = 0.9\n",
      "Episodio 41: reward = -123.87581888666666, exploración = 0.9\n",
      "Episodio 42: reward = -90.47897903500001, exploración = 0.9\n",
      "Episodio 43: reward = -121.36393762, exploración = 0.9\n",
      "Episodio 44: reward = -100.35267408333335, exploración = 0.9\n",
      "Episodio 45: reward = -93.76305627833332, exploración = 0.9\n",
      "Episodio 46: reward = -158.712361265, exploración = 0.9\n",
      "Episodio 47: reward = -155.59664701, exploración = 0.9\n",
      "Episodio 48: reward = -74.64120308166667, exploración = 0.9\n",
      "Episodio 49: reward = -107.277930885, exploración = 0.9\n",
      "Episodio 50: reward = -126.79936759833336, exploración = 0.9\n",
      "Episodio 51: reward = -151.59060096333334, exploración = 0.9\n",
      "Episodio 52: reward = -158.169381575, exploración = 0.9\n",
      "Episodio 53: reward = -183.29438370666668, exploración = 0.9\n",
      "Episodio 54: reward = -166.3094937716667, exploración = 0.9\n",
      "Episodio 55: reward = -96.62695406333333, exploración = 0.9\n",
      "Episodio 56: reward = -106.94842094333336, exploración = 0.9\n",
      "Episodio 57: reward = -111.98220382666668, exploración = 0.9\n",
      "Episodio 58: reward = -93.66012117166667, exploración = 0.9\n",
      "Episodio 59: reward = -79.15138174166667, exploración = 0.9\n",
      "Episodio 60: reward = -148.4722156616667, exploración = 0.9\n",
      "Episodio 61: reward = -188.40908455666667, exploración = 0.9\n",
      "Episodio 62: reward = -74.94207105333334, exploración = 0.9\n",
      "Episodio 63: reward = -75.54238075, exploración = 0.9\n",
      "Episodio 64: reward = -129.16534121166669, exploración = 0.9\n",
      "Episodio 65: reward = -93.97723359166667, exploración = 0.9\n",
      "Episodio 66: reward = -90.34836639833333, exploración = 0.9\n",
      "Episodio 67: reward = -67.45352365833334, exploración = 0.9\n",
      "Episodio 68: reward = -84.13993419333333, exploración = 0.9\n",
      "Episodio 69: reward = -67.050114285, exploración = 0.9\n",
      "Episodio 70: reward = -94.98366387, exploración = 0.9\n",
      "Episodio 71: reward = -91.77978714166667, exploración = 0.9\n",
      "Episodio 72: reward = -72.14598197666666, exploración = 0.9\n",
      "Episodio 73: reward = -74.47158565166667, exploración = 0.9\n",
      "Episodio 74: reward = -79.36038533666668, exploración = 0.9\n",
      "Episodio 75: reward = -209.64057740500002, exploración = 0.9\n",
      "Episodio 76: reward = -99.72344313500001, exploración = 0.9\n",
      "Episodio 77: reward = -125.82399776, exploración = 0.9\n",
      "Episodio 78: reward = -78.02228739833333, exploración = 0.9\n",
      "Episodio 79: reward = -58.716465675, exploración = 0.9\n",
      "Episodio 80: reward = -65.15799248166667, exploración = 0.9\n",
      "Episodio 81: reward = -140.91368253166667, exploración = 0.9\n",
      "Episodio 82: reward = -48.04247924666666, exploración = 0.9\n",
      "Episodio 83: reward = -153.40618443666668, exploración = 0.9\n",
      "Episodio 84: reward = -152.7129396166667, exploración = 0.9\n",
      "Episodio 85: reward = -191.06125922166666, exploración = 0.9\n",
      "Episodio 86: reward = -67.17557021333333, exploración = 0.9\n",
      "Episodio 87: reward = -118.10961915333337, exploración = 0.9\n",
      "Episodio 88: reward = -90.68426556833333, exploración = 0.9\n",
      "Episodio 89: reward = -113.99170812666667, exploración = 0.9\n",
      "Episodio 90: reward = -168.23745630500002, exploración = 0.9\n",
      "Episodio 91: reward = -69.40660242666667, exploración = 0.9\n",
      "Episodio 92: reward = -118.47710406999997, exploración = 0.9\n",
      "Episodio 93: reward = -168.55767981333332, exploración = 0.9\n",
      "Episodio 94: reward = -79.75235452333334, exploración = 0.9\n",
      "Episodio 95: reward = -184.8082980966667, exploración = 0.9\n",
      "Episodio 96: reward = -142.04985474333333, exploración = 0.9\n",
      "Episodio 97: reward = -158.02519478666667, exploración = 0.9\n",
      "Episodio 98: reward = -152.0834562533333, exploración = 0.9\n",
      "Episodio 99: reward = -58.89090126833334, exploración = 0.9\n",
      "Episodio 100: reward = -101.69187167666666, exploración = 0.9\n",
      "Media de reward en episodios 1 a 100: -113.74773396888335\n",
      "Episodio 101: reward = -65.23879569000002, exploración = 0.9\n",
      "Episodio 102: reward = -206.47819739833335, exploración = 0.9\n",
      "Episodio 103: reward = -71.130176765, exploración = 0.9\n",
      "Episodio 104: reward = -147.18254125166666, exploración = 0.9\n",
      "Episodio 105: reward = -88.03171838833335, exploración = 0.9\n",
      "Episodio 106: reward = -44.25815731166667, exploración = 0.9\n",
      "Episodio 107: reward = -99.81519787333335, exploración = 0.9\n",
      "Episodio 108: reward = -121.92335966000002, exploración = 0.9\n",
      "Episodio 109: reward = -81.91152859166667, exploración = 0.9\n",
      "Episodio 110: reward = -85.01247142833336, exploración = 0.9\n",
      "Episodio 111: reward = -172.58000791166666, exploración = 0.9\n",
      "Episodio 112: reward = -155.54255931, exploración = 0.9\n",
      "Episodio 113: reward = -115.69355416666667, exploración = 0.9\n",
      "Episodio 114: reward = -110.72589199166666, exploración = 0.9\n",
      "Episodio 115: reward = -84.44016277, exploración = 0.9\n",
      "Episodio 116: reward = -97.72313277, exploración = 0.9\n",
      "Episodio 117: reward = -128.74123789833334, exploración = 0.9\n",
      "Episodio 118: reward = -96.27735640999998, exploración = 0.9\n",
      "Episodio 119: reward = -88.61272516833333, exploración = 0.9\n",
      "Episodio 120: reward = -41.187827760000005, exploración = 0.9\n",
      "Episodio 121: reward = -228.76160394833335, exploración = 0.9\n",
      "Episodio 122: reward = -80.90888466333334, exploración = 0.9\n",
      "Episodio 123: reward = -106.363951955, exploración = 0.9\n",
      "Episodio 124: reward = -118.05916212833333, exploración = 0.9\n",
      "Episodio 125: reward = -123.56184932166667, exploración = 0.9\n",
      "Episodio 126: reward = -151.05447948833339, exploración = 0.9\n",
      "Episodio 127: reward = -119.62225071000002, exploración = 0.9\n",
      "Episodio 128: reward = -121.27648501833332, exploración = 0.9\n",
      "Episodio 129: reward = -54.196009276666665, exploración = 0.9\n",
      "Episodio 130: reward = -63.36757747666667, exploración = 0.9\n",
      "Episodio 131: reward = -79.85026901666666, exploración = 0.9\n",
      "Episodio 132: reward = -195.52507141666666, exploración = 0.9\n",
      "Episodio 133: reward = -77.900728055, exploración = 0.9\n",
      "Episodio 134: reward = -113.25631473833333, exploración = 0.9\n",
      "Episodio 135: reward = -51.860359426666676, exploración = 0.9\n",
      "Episodio 136: reward = -58.725155728333334, exploración = 0.9\n",
      "Episodio 137: reward = -65.10018114833333, exploración = 0.9\n",
      "Episodio 138: reward = -56.31312532166667, exploración = 0.9\n",
      "Episodio 139: reward = -104.98336828500001, exploración = 0.9\n",
      "Episodio 140: reward = -166.56374420333333, exploración = 0.9\n",
      "Episodio 141: reward = -81.58421759000001, exploración = 0.9\n",
      "Episodio 142: reward = -126.68624231499999, exploración = 0.9\n",
      "Episodio 143: reward = -125.27097913, exploración = 0.9\n",
      "Episodio 144: reward = -112.71757428166666, exploración = 0.9\n",
      "Episodio 145: reward = -99.26874075, exploración = 0.9\n",
      "Episodio 146: reward = -156.31199740166667, exploración = 0.9\n",
      "Episodio 147: reward = -104.40980459166666, exploración = 0.9\n",
      "Episodio 148: reward = -44.27911390833333, exploración = 0.9\n",
      "Episodio 149: reward = -54.52913822833334, exploración = 0.9\n",
      "Episodio 150: reward = -90.67588531500002, exploración = 0.9\n",
      "Episodio 151: reward = -114.39373404333334, exploración = 0.9\n",
      "Episodio 152: reward = -99.67464189666667, exploración = 0.9\n",
      "Episodio 153: reward = -80.64102526666667, exploración = 0.9\n",
      "Episodio 154: reward = -89.0509578, exploración = 0.9\n",
      "Episodio 155: reward = -77.53385528166667, exploración = 0.9\n",
      "Episodio 156: reward = -103.73743510666667, exploración = 0.9\n",
      "Episodio 157: reward = -81.47576513833334, exploración = 0.9\n",
      "Episodio 158: reward = -87.06283034166667, exploración = 0.9\n",
      "Episodio 159: reward = -76.84603723666667, exploración = 0.9\n",
      "Episodio 160: reward = -51.82791598, exploración = 0.9\n",
      "Episodio 161: reward = -103.75548904499999, exploración = 0.9\n",
      "Episodio 162: reward = -126.896771615, exploración = 0.9\n",
      "Episodio 163: reward = -156.10910324500003, exploración = 0.9\n",
      "Episodio 164: reward = -134.00885004833333, exploración = 0.9\n",
      "Episodio 165: reward = -90.36111883000001, exploración = 0.9\n",
      "Episodio 166: reward = -80.39473548666668, exploración = 0.9\n",
      "Episodio 167: reward = -94.30502162666667, exploración = 0.9\n",
      "Episodio 168: reward = -73.77191021166666, exploración = 0.9\n",
      "Episodio 169: reward = -154.67726378333336, exploración = 0.9\n",
      "Episodio 170: reward = -70.65208858333332, exploración = 0.9\n",
      "Episodio 171: reward = -66.76914045333334, exploración = 0.9\n",
      "Episodio 172: reward = -106.39798985499999, exploración = 0.9\n",
      "Episodio 173: reward = -129.954713115, exploración = 0.9\n",
      "Episodio 174: reward = -97.32306692, exploración = 0.9\n",
      "Episodio 175: reward = -151.6940065516667, exploración = 0.9\n",
      "Episodio 176: reward = -143.63874883166667, exploración = 0.9\n",
      "Episodio 177: reward = -95.731737805, exploración = 0.9\n",
      "Episodio 178: reward = -151.31755930166668, exploración = 0.9\n",
      "Episodio 179: reward = -71.43324734, exploración = 0.9\n",
      "Episodio 180: reward = -70.85944614333334, exploración = 0.9\n",
      "Episodio 181: reward = -96.62839161499998, exploración = 0.9\n",
      "Episodio 182: reward = -65.53344512999999, exploración = 0.9\n",
      "Episodio 183: reward = -60.598277655000004, exploración = 0.9\n",
      "Episodio 184: reward = -132.31795063833334, exploración = 0.9\n",
      "Episodio 185: reward = -41.807965081666666, exploración = 0.9\n",
      "Episodio 186: reward = -79.29518005166666, exploración = 0.9\n",
      "Episodio 187: reward = -141.606256855, exploración = 0.9\n",
      "Episodio 188: reward = -139.23873640166667, exploración = 0.9\n",
      "Episodio 189: reward = -91.56528606666667, exploración = 0.9\n",
      "Episodio 190: reward = -95.62021690666667, exploración = 0.9\n",
      "Episodio 191: reward = -163.12576185, exploración = 0.9\n",
      "Episodio 192: reward = -71.6250703, exploración = 0.9\n",
      "Episodio 193: reward = -67.85268299833334, exploración = 0.9\n",
      "Episodio 194: reward = -56.28079525833334, exploración = 0.9\n",
      "Episodio 195: reward = -138.93676163499998, exploración = 0.9\n",
      "Episodio 196: reward = -123.10800549333334, exploración = 0.9\n",
      "Episodio 197: reward = -106.71182767666667, exploración = 0.9\n",
      "Episodio 198: reward = -115.51274364333332, exploración = 0.9\n",
      "Episodio 199: reward = -72.20589939833334, exploración = 0.9\n",
      "Episodio 200: reward = -108.25946801666667, exploración = 0.9\n",
      "Media de reward en episodios 101 a 200: -102.35617794908335\n",
      "Episodio 201: reward = -85.16304585500001, exploración = 0.9\n",
      "Episodio 202: reward = -177.83091749833335, exploración = 0.9\n",
      "Episodio 203: reward = -68.41554185833334, exploración = 0.9\n",
      "Episodio 204: reward = -107.33471812000002, exploración = 0.9\n",
      "Episodio 205: reward = -61.75591949333334, exploración = 0.9\n",
      "Episodio 206: reward = -155.5342683416667, exploración = 0.9\n",
      "Episodio 207: reward = -179.67861588333335, exploración = 0.9\n",
      "Episodio 208: reward = -104.45966772333335, exploración = 0.9\n",
      "Episodio 209: reward = -119.90365518666667, exploración = 0.9\n",
      "Episodio 210: reward = -70.26997185333335, exploración = 0.9\n",
      "Episodio 211: reward = -63.27398174333333, exploración = 0.9\n",
      "Episodio 212: reward = -180.62106286833335, exploración = 0.9\n",
      "Episodio 213: reward = -92.122419665, exploración = 0.9\n",
      "Episodio 214: reward = -82.5207126, exploración = 0.9\n",
      "Episodio 215: reward = -72.66868544, exploración = 0.9\n",
      "Episodio 216: reward = -130.58861027999998, exploración = 0.9\n",
      "Episodio 217: reward = -50.548089133333335, exploración = 0.9\n",
      "Episodio 218: reward = -68.817954415, exploración = 0.9\n",
      "Episodio 219: reward = -65.06763949666667, exploración = 0.9\n",
      "Episodio 220: reward = -105.75864379666666, exploración = 0.9\n",
      "Episodio 221: reward = -89.02900632666667, exploración = 0.9\n",
      "Episodio 222: reward = -149.56922257333332, exploración = 0.9\n",
      "Episodio 223: reward = -127.37458726333337, exploración = 0.9\n",
      "Episodio 224: reward = -113.73090081666666, exploración = 0.9\n",
      "Episodio 225: reward = -68.04038564833334, exploración = 0.9\n",
      "Episodio 226: reward = -113.16900299333336, exploración = 0.9\n",
      "Episodio 227: reward = -81.65082598666669, exploración = 0.9\n",
      "Episodio 228: reward = -72.533358005, exploración = 0.9\n",
      "Episodio 229: reward = -92.01804491000001, exploración = 0.9\n",
      "Episodio 230: reward = -74.94916181333335, exploración = 0.9\n",
      "Episodio 231: reward = -57.28996558, exploración = 0.9\n",
      "Episodio 232: reward = -184.87828154333334, exploración = 0.9\n",
      "Episodio 233: reward = -76.02434862833333, exploración = 0.9\n",
      "Episodio 234: reward = -120.63770319666666, exploración = 0.9\n",
      "Episodio 235: reward = -80.35823035500002, exploración = 0.9\n",
      "Episodio 236: reward = -126.52642269333336, exploración = 0.9\n",
      "Episodio 237: reward = -86.96791415166666, exploración = 0.9\n",
      "Episodio 238: reward = -84.66393278999999, exploración = 0.9\n",
      "Episodio 239: reward = -110.83932380666667, exploración = 0.9\n",
      "Episodio 240: reward = -50.33189268500001, exploración = 0.9\n",
      "Episodio 241: reward = -154.12577057833334, exploración = 0.9\n",
      "Episodio 242: reward = -83.06009014666667, exploración = 0.9\n",
      "Episodio 243: reward = -62.91834211833333, exploración = 0.9\n",
      "Episodio 244: reward = -178.92342454833334, exploración = 0.9\n",
      "Episodio 245: reward = -113.01409195833332, exploración = 0.9\n",
      "Episodio 246: reward = -125.41581596500001, exploración = 0.9\n",
      "Episodio 247: reward = -142.96352130166667, exploración = 0.9\n",
      "Episodio 248: reward = -133.95111191, exploración = 0.9\n",
      "Episodio 249: reward = -80.43629705333332, exploración = 0.9\n",
      "Episodio 250: reward = -61.23507670166667, exploración = 0.9\n",
      "Episodio 251: reward = -70.43584821333334, exploración = 0.9\n",
      "Episodio 252: reward = -155.11467309333335, exploración = 0.9\n",
      "Episodio 253: reward = -142.57041975833334, exploración = 0.9\n",
      "Episodio 254: reward = -70.58096948166667, exploración = 0.9\n",
      "Episodio 255: reward = -209.61808742166667, exploración = 0.9\n",
      "Episodio 256: reward = -74.20682715000001, exploración = 0.9\n",
      "Episodio 257: reward = -181.420433425, exploración = 0.9\n",
      "Episodio 258: reward = -127.80812982333335, exploración = 0.9\n",
      "Episodio 259: reward = -71.86771863666667, exploración = 0.9\n",
      "Episodio 260: reward = -74.31754708666665, exploración = 0.9\n",
      "Episodio 261: reward = -76.35603146333332, exploración = 0.9\n",
      "Episodio 262: reward = -53.680500345000006, exploración = 0.9\n",
      "Episodio 263: reward = -105.18016742500001, exploración = 0.9\n",
      "Episodio 264: reward = -70.00080901, exploración = 0.9\n",
      "Episodio 265: reward = -223.45576724333336, exploración = 0.9\n",
      "Episodio 266: reward = -117.06839399, exploración = 0.9\n",
      "Episodio 267: reward = -159.48488641666665, exploración = 0.9\n",
      "Episodio 268: reward = -72.64029922333333, exploración = 0.9\n",
      "Episodio 269: reward = -46.40026634333333, exploración = 0.9\n",
      "Episodio 270: reward = -56.51532887333333, exploración = 0.9\n",
      "Episodio 271: reward = -56.437262081666674, exploración = 0.9\n",
      "Episodio 272: reward = -75.42211569, exploración = 0.9\n",
      "Episodio 273: reward = -155.86714142000002, exploración = 0.9\n",
      "Episodio 274: reward = -80.38104889499999, exploración = 0.9\n",
      "Episodio 275: reward = -81.30866969, exploración = 0.9\n",
      "Episodio 276: reward = -79.283472315, exploración = 0.9\n",
      "Episodio 277: reward = -80.53817207333333, exploración = 0.9\n",
      "Episodio 278: reward = -66.14267648666664, exploración = 0.9\n",
      "Episodio 279: reward = -86.39406394666668, exploración = 0.9\n",
      "Episodio 280: reward = -62.334381583333325, exploración = 0.9\n",
      "Episodio 281: reward = -68.56833607666668, exploración = 0.9\n",
      "Episodio 282: reward = -71.36105705166668, exploración = 0.9\n",
      "Episodio 283: reward = -39.451170678333334, exploración = 0.9\n",
      "Episodio 284: reward = -67.705527835, exploración = 0.9\n",
      "Episodio 285: reward = -92.30770451000001, exploración = 0.9\n",
      "Episodio 286: reward = -133.33225178666666, exploración = 0.9\n",
      "Episodio 287: reward = -67.716419645, exploración = 0.9\n",
      "Episodio 288: reward = -96.82142802333334, exploración = 0.9\n",
      "Episodio 289: reward = -91.33019438166667, exploración = 0.9\n",
      "Episodio 290: reward = -100.95591026166667, exploración = 0.9\n",
      "Episodio 291: reward = -49.44033725833333, exploración = 0.9\n",
      "Episodio 292: reward = -102.16676149166668, exploración = 0.9\n",
      "Episodio 293: reward = -104.39936248166669, exploración = 0.9\n",
      "Episodio 294: reward = -109.69971668166669, exploración = 0.9\n",
      "Episodio 295: reward = -111.43549829666665, exploración = 0.9\n",
      "Episodio 296: reward = -135.79918120333335, exploración = 0.9\n",
      "Episodio 297: reward = -133.28314548, exploración = 0.9\n",
      "Episodio 298: reward = -133.16274098833333, exploración = 0.9\n",
      "Episodio 299: reward = -143.62116136, exploración = 0.9\n",
      "Episodio 300: reward = -117.75069884, exploración = 0.9\n",
      "Media de reward en episodios 201 a 300: -100.92100888236666\n",
      "Episodio 301: reward = -86.97011813333334, exploración = 0.9\n",
      "Episodio 302: reward = -126.33020479999998, exploración = 0.9\n",
      "Episodio 303: reward = -98.65244064833335, exploración = 0.9\n",
      "Episodio 304: reward = -77.04798379500001, exploración = 0.9\n",
      "Episodio 305: reward = -78.075721305, exploración = 0.9\n",
      "Episodio 306: reward = -143.68091707333335, exploración = 0.9\n",
      "Episodio 307: reward = -146.71499411833332, exploración = 0.9\n",
      "Episodio 308: reward = -124.19147148166667, exploración = 0.9\n",
      "Episodio 309: reward = -72.92281483666666, exploración = 0.9\n",
      "Episodio 310: reward = -129.08246491666668, exploración = 0.9\n",
      "Episodio 311: reward = -189.78765223166664, exploración = 0.9\n",
      "Episodio 312: reward = -77.99391285666667, exploración = 0.9\n",
      "Episodio 313: reward = -76.85028717833333, exploración = 0.9\n",
      "Episodio 314: reward = -109.68584521833331, exploración = 0.9\n",
      "Episodio 315: reward = -50.69295921166666, exploración = 0.9\n",
      "Episodio 316: reward = -81.04175580500001, exploración = 0.9\n",
      "Episodio 317: reward = -98.522705165, exploración = 0.9\n",
      "Episodio 318: reward = -61.362009096666675, exploración = 0.9\n",
      "Episodio 319: reward = -130.47142043833333, exploración = 0.9\n",
      "Episodio 320: reward = -121.34893344666668, exploración = 0.9\n",
      "Episodio 321: reward = -80.88911581, exploración = 0.9\n",
      "Episodio 322: reward = -121.501542355, exploración = 0.9\n",
      "Episodio 323: reward = -119.34900708833334, exploración = 0.9\n",
      "Episodio 324: reward = -73.85737037666667, exploración = 0.9\n",
      "Episodio 325: reward = -79.100674885, exploración = 0.9\n",
      "Episodio 326: reward = -162.1657889133333, exploración = 0.9\n",
      "Episodio 327: reward = -40.00465516833334, exploración = 0.9\n",
      "Episodio 328: reward = -205.26578306833335, exploración = 0.9\n",
      "Episodio 329: reward = -94.120992395, exploración = 0.9\n",
      "Episodio 330: reward = -81.68234293166665, exploración = 0.9\n",
      "Episodio 331: reward = -73.18451407666667, exploración = 0.9\n",
      "Episodio 332: reward = -104.26343196333332, exploración = 0.9\n",
      "Episodio 333: reward = -101.838758955, exploración = 0.9\n",
      "Episodio 334: reward = -162.84266884666667, exploración = 0.9\n",
      "Episodio 335: reward = -126.21408106333332, exploración = 0.9\n",
      "Episodio 336: reward = -72.63570285, exploración = 0.9\n",
      "Episodio 337: reward = -48.95923943833334, exploración = 0.9\n",
      "Episodio 338: reward = -158.28334176333334, exploración = 0.9\n",
      "Episodio 339: reward = -134.30978571, exploración = 0.9\n",
      "Episodio 340: reward = -126.875062345, exploración = 0.9\n",
      "Episodio 341: reward = -127.94254059000001, exploración = 0.9\n",
      "Episodio 342: reward = -88.46114268666668, exploración = 0.9\n",
      "Episodio 343: reward = -172.63489861333335, exploración = 0.9\n",
      "Episodio 344: reward = -163.61780798, exploración = 0.9\n",
      "Episodio 345: reward = -255.08938549833331, exploración = 0.9\n",
      "Episodio 346: reward = -180.82431158833333, exploración = 0.9\n",
      "Episodio 347: reward = -98.81594571833335, exploración = 0.9\n",
      "Episodio 348: reward = -70.58443409333333, exploración = 0.9\n",
      "Episodio 349: reward = -69.80201082333335, exploración = 0.9\n",
      "Episodio 350: reward = -128.03633466500003, exploración = 0.9\n",
      "Episodio 351: reward = -63.879437955, exploración = 0.9\n",
      "Episodio 352: reward = -118.30454319500001, exploración = 0.9\n",
      "Episodio 353: reward = -60.71654648500001, exploración = 0.9\n",
      "Episodio 354: reward = -101.90556315666667, exploración = 0.9\n",
      "Episodio 355: reward = -98.48467489333333, exploración = 0.9\n",
      "Episodio 356: reward = -48.28394419666667, exploración = 0.9\n",
      "Episodio 357: reward = -166.628720955, exploración = 0.9\n",
      "Episodio 358: reward = -128.12667442166665, exploración = 0.9\n",
      "Episodio 359: reward = -89.82124660999999, exploración = 0.9\n",
      "Episodio 360: reward = -106.90960575833336, exploración = 0.9\n",
      "Episodio 361: reward = -140.52040075000002, exploración = 0.9\n",
      "Episodio 362: reward = -84.14082043666667, exploración = 0.9\n",
      "Episodio 363: reward = -97.65928328666666, exploración = 0.9\n",
      "Episodio 364: reward = -106.40883518333334, exploración = 0.9\n",
      "Episodio 365: reward = -90.46915884833335, exploración = 0.9\n",
      "Episodio 366: reward = -67.415548075, exploración = 0.9\n",
      "Episodio 367: reward = -171.65603320666668, exploración = 0.9\n",
      "Episodio 368: reward = -121.48150592333334, exploración = 0.9\n",
      "Episodio 369: reward = -113.0043081, exploración = 0.9\n",
      "Episodio 370: reward = -79.93058294, exploración = 0.9\n",
      "Episodio 371: reward = -94.763166545, exploración = 0.9\n",
      "Episodio 372: reward = -162.195488695, exploración = 0.9\n",
      "Episodio 373: reward = -91.722616375, exploración = 0.9\n",
      "Episodio 374: reward = -80.00219682166667, exploración = 0.9\n",
      "Episodio 375: reward = -78.58805322, exploración = 0.9\n",
      "Episodio 376: reward = -133.16352837666668, exploración = 0.9\n",
      "Episodio 377: reward = -58.467468108333335, exploración = 0.9\n",
      "Episodio 378: reward = -69.884748785, exploración = 0.9\n",
      "Episodio 379: reward = -87.17235378166666, exploración = 0.9\n",
      "Episodio 380: reward = -100.82974459666667, exploración = 0.9\n",
      "Episodio 381: reward = -94.46563987166667, exploración = 0.9\n",
      "Episodio 382: reward = -89.61737615333335, exploración = 0.9\n",
      "Episodio 383: reward = -125.732836545, exploración = 0.9\n",
      "Episodio 384: reward = -66.47631610833332, exploración = 0.9\n",
      "Episodio 385: reward = -60.373941908333336, exploración = 0.9\n",
      "Episodio 386: reward = -144.23501469166666, exploración = 0.9\n",
      "Episodio 387: reward = -118.12068337833333, exploración = 0.9\n",
      "Episodio 388: reward = -130.343423025, exploración = 0.9\n",
      "Episodio 389: reward = -114.164855555, exploración = 0.9\n",
      "Episodio 390: reward = -56.26523735166667, exploración = 0.9\n",
      "Episodio 391: reward = -98.547091175, exploración = 0.9\n",
      "Episodio 392: reward = -196.42941519833334, exploración = 0.9\n",
      "Episodio 393: reward = -64.53700303333333, exploración = 0.9\n",
      "Episodio 394: reward = -90.00203531166667, exploración = 0.9\n",
      "Episodio 395: reward = -63.650959786666654, exploración = 0.9\n",
      "Episodio 396: reward = -97.07507676333333, exploración = 0.9\n",
      "Episodio 397: reward = -127.49122750666666, exploración = 0.9\n",
      "Episodio 398: reward = -132.83714709, exploración = 0.9\n",
      "Episodio 399: reward = -94.48586059000002, exploración = 0.9\n",
      "Episodio 400: reward = -70.03192334833334, exploración = 0.9\n",
      "Media de reward en episodios 301 a 400: -106.51999148090003\n",
      "Episodio 401: reward = -81.549848655, exploración = 0.9\n",
      "Episodio 402: reward = -138.898766505, exploración = 0.9\n",
      "Episodio 403: reward = -201.09857890166666, exploración = 0.9\n",
      "Episodio 404: reward = -218.35725252333333, exploración = 0.9\n",
      "Episodio 405: reward = -90.35575070166666, exploración = 0.9\n",
      "Episodio 406: reward = -129.128017215, exploración = 0.9\n",
      "Episodio 407: reward = -116.81442508833334, exploración = 0.9\n",
      "Episodio 408: reward = -130.77248655, exploración = 0.9\n",
      "Episodio 409: reward = -114.68049223166668, exploración = 0.9\n",
      "Episodio 410: reward = -183.49065565166666, exploración = 0.9\n",
      "Episodio 411: reward = -72.83634484333334, exploración = 0.9\n",
      "Episodio 412: reward = -189.93093902833334, exploración = 0.9\n",
      "Episodio 413: reward = -147.71398625666666, exploración = 0.9\n",
      "Episodio 414: reward = -65.25184793333334, exploración = 0.9\n",
      "Episodio 415: reward = -99.31722574, exploración = 0.9\n",
      "Episodio 416: reward = -104.34031365500002, exploración = 0.9\n",
      "Episodio 417: reward = -91.08321421166669, exploración = 0.9\n",
      "Episodio 418: reward = -93.47826261, exploración = 0.9\n",
      "Episodio 419: reward = -89.41241018833335, exploración = 0.9\n",
      "Episodio 420: reward = -81.64351687000001, exploración = 0.9\n",
      "Episodio 421: reward = -104.94459661166665, exploración = 0.9\n",
      "Episodio 422: reward = -163.15088189833335, exploración = 0.9\n",
      "Episodio 423: reward = -118.26123487666666, exploración = 0.9\n",
      "Episodio 424: reward = -71.57939856166666, exploración = 0.9\n",
      "Episodio 425: reward = -100.10521204166666, exploración = 0.9\n",
      "Episodio 426: reward = -100.50477742666666, exploración = 0.9\n",
      "Episodio 427: reward = -52.438591685000006, exploración = 0.9\n",
      "Episodio 428: reward = -76.89501485833333, exploración = 0.9\n",
      "Episodio 429: reward = -233.62580610333333, exploración = 0.9\n",
      "Episodio 430: reward = -82.20373957833334, exploración = 0.9\n",
      "Episodio 431: reward = -98.66126882833335, exploración = 0.9\n",
      "Episodio 432: reward = -64.73410109166667, exploración = 0.9\n",
      "Episodio 433: reward = -92.57228919166667, exploración = 0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     action \u001b[38;5;241m=\u001b[39m optimal_policy(state, Q)\n\u001b[1;32m---> 37\u001b[0m next_obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m next_state \u001b[38;5;241m=\u001b[39m get_state(next_obs)\n\u001b[0;32m     39\u001b[0m action_idx \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mindex(action)\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\descent_env.py:185\u001b[0m, in \u001b[0;36mDescentEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    183\u001b[0m action_frequency \u001b[38;5;241m=\u001b[39m ACTION_FREQUENCY\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_frequency):\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame()\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\simulation\\simulation.py:107\u001b[0m, in \u001b[0;36mSimulation.step\u001b[1;34m(self, dt_increment)\u001b[0m\n\u001b[0;32m    105\u001b[0m plotter\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    106\u001b[0m datalog\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 107\u001b[0m \u001b[43mhooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Determine interval towards next timestep                \u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimdt \u001b[38;5;241m=\u001b[39m simtime\u001b[38;5;241m.\u001b[39mstep(dt_increment)\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\timedfunction.py:13\u001b[0m, in \u001b[0;36m_Hook.trigger\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrigger\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 13\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\timedfunction.py:51\u001b[0m, in \u001b[0;36mtimed_function.<locals>.deco.<locals>.callback\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fobj)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcallback\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timer\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mfobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_act\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\funcobject.py:32\u001b[0m, in \u001b[0;36mFuncObject.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\performance\\openap\\perfoap.py:249\u001b[0m, in \u001b[0;36mOpenAP.update\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuelflow[idx_fixwing] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengnum[idx_fixwing] \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_coeff_a[idx_fixwing] \u001b[38;5;241m*\u001b[39m thrustratio_fixwing \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_coeff_b[idx_fixwing] \u001b[38;5;241m*\u001b[39m thrustratio_fixwing\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_coeff_c[idx_fixwing]\n\u001b[0;32m    246\u001b[0m )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# ----- update max acceleration ----\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_axmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# TODO: implement thrust computation for rotor aircraft\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# idx_rotor = np.where(self.lifttype==coeff.LIFT_ROTOR)[0]\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# self.thrust[idx_rotor] = 0\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# update bank angle, due to phase change\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbank \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase \u001b[38;5;241m==\u001b[39m ph\u001b[38;5;241m.\u001b[39mGD), \u001b[38;5;241m15\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbank)\n",
      "File \u001b[1;32mc:\\Users\\agusp\\OneDrive\\Escritorio\\ORT Git\\Inteligencia Artificial\\Obl_IA\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\performance\\openap\\perfoap.py:397\u001b[0m, in \u001b[0;36mOpenAP.calc_axmax\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m vmin, vmax\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmin[mask], vmax[mask]\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalc_axmax\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# accelerations depending on phase and wing type\u001b[39;00m\n\u001b[0;32m    399\u001b[0m     axmax_fixwing_ground \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    400\u001b[0m     axmax_rotor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.5\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_reward = 0\n",
    "rewards = []\n",
    "max_steps = 1\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "def get_explore_prob(i):\n",
    "    # Entre 0 y 1000: explorar 90%\n",
    "    # Entre 1001 y 2000: explotar 80%\n",
    "    # Entre 2001 y 3000: explorar 90%\n",
    "    # Entre 3001 y 4000: explotar 80%\n",
    "    if 0 <= i <= 1000:\n",
    "        return 0.9\n",
    "    elif 1001 <= i <= 2000:\n",
    "        return 0.2\n",
    "    elif 2001 <= i <= 3000:\n",
    "        return 0.9\n",
    "    elif 3001 <= i <= 4000:\n",
    "        return 0.2\n",
    "    else:\n",
    "        return 0.1  # default\n",
    "\n",
    "while i < 4000:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action = optimal_policy(state, Q)\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        action_idx = actions.index(action)\n",
    "        Q[state][action_idx] = Q[state][action_idx] + 0.9 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    print(f\"Episodio {i+1}: reward = {episode_reward}, exploración = {prob}\")\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from C:\\Users\\npere\\bluesky\\settings.cfg\n",
      "Reading magnetic variation data\n",
      "Loading global navigation database...\n",
      "Reading cache: C:\\Users\\npere\\bluesky\\cache\\navdata.p\n",
      "Attempt to reimplement AREA from <bound method Area.set_area of <bluesky.plugins.area.Area object at 0x0000028E21095E40>> to <bound method Area.set_area of <bluesky.plugins.area.Area object at 0x0000028E21095E40>>\n",
      "Attempt to reimplement EXP from <function init_plugin.<locals>.<lambda> at 0x0000028E21005630> to <function init_plugin.<locals>.<lambda> at 0x0000028E45E94820>\n",
      "Attempt to reimplement TAXI from <bound method Area.set_taxi of <bluesky.plugins.area.Area object at 0x0000028E21095E40>> to <bound method Area.set_taxi of <bluesky.plugins.area.Area object at 0x0000028E21095E40>>\n",
      "Successfully loaded plugin AREA\n",
      "Attempt to reimplement DATAFEED from <bound method Modesbeast.toggle of <bluesky.plugins.adsbfeed.Modesbeast object at 0x0000028E21097790>> to <bound method Modesbeast.toggle of <bluesky.plugins.adsbfeed.Modesbeast object at 0x0000028E3AA74070>>\n",
      "Successfully loaded plugin DATAFEED\n",
      "Attempt to reimplement PLUGINS from <function init.<locals>.manage at 0x0000028E21099360> to <function init.<locals>.manage at 0x0000028E45E955A0>\n",
      "Attempt to reimplement ADDNODES from <bound method Node.addnodes of <bluesky.network.detached.Node object at 0x0000028E20FF2350>> to <bound method Node.addnodes of <bluesky.network.detached.Node object at 0x0000028E3AB1E110>>\n",
      "Attempt to reimplement AIRWAY from <bound method Traffic.airwaycmd of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.airwaycmd of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement ATALT from <bound method Condition.ataltcmd of <bluesky.traffic.conditional.Condition object at 0x0000028E20FF2DD0>> to <bound method Condition.ataltcmd of <bluesky.traffic.conditional.Condition object at 0x0000028E20FF2DD0>>\n",
      "Attempt to reimplement ATDIST from <bound method Condition.atdistcmd of <bluesky.traffic.conditional.Condition object at 0x0000028E20FF2DD0>> to <bound method Condition.atdistcmd of <bluesky.traffic.conditional.Condition object at 0x0000028E20FF2DD0>>\n",
      "Attempt to reimplement ATSPD from <bound method Condition.atspdcmd of <bluesky.traffic.conditional.Condition object at 0x0000028E20FF2DD0>> to <bound method Condition.atspdcmd of <bluesky.traffic.conditional.Condition object at 0x0000028E20FF2DD0>>\n",
      "Attempt to reimplement BANK from <bound method Traffic.setbanklim of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.setbanklim of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement BATCH from <bound method Simulation.batch of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.batch of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement BLUESKY from <function singbluesky at 0x0000028E1C938940> to <function singbluesky at 0x0000028E1C938940>\n",
      "Attempt to reimplement BENCHMARK from <bound method Simulation.benchmark of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.benchmark of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement BOX from <function initbasecmds.<locals>.<lambda> at 0x0000028E21039240> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E955A0>\n",
      "Attempt to reimplement CALC from <function calculator at 0x0000028E1C9388B0> to <function calculator at 0x0000028E1C9388B0>\n",
      "Attempt to reimplement CASMACHTHR from <function casmachthr at 0x0000028E1C8E0C10> to <function casmachthr at 0x0000028E1C8E0C10>\n",
      "Attempt to reimplement CD from <function setscenpath at 0x0000028E1C938A60> to <function setscenpath at 0x0000028E1C938A60>\n",
      "Attempt to reimplement CIRCLE from <function initbasecmds.<locals>.<lambda> at 0x0000028E210993F0> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94940>\n",
      "Attempt to reimplement CLRCRECMD from <bound method Traffic.clrcrecmd of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.clrcrecmd of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement CRE from <bound method Traffic.cre of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.cre of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement CRECMD from <bound method Traffic.crecmd of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.crecmd of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement CRECONFS from <bound method Traffic.creconfs of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.creconfs of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement DATE from <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement DEFWPT from <bound method Navdatabase.defwpt of <bluesky.navdatabase.navdatabase.Navdatabase object at 0x0000028E1C974520>> to <bound method Navdatabase.defwpt of <bluesky.navdatabase.navdatabase.Navdatabase object at 0x0000028E210B3760>>\n",
      "Attempt to reimplement DEL from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099870> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94DC0>\n",
      "Attempt to reimplement DIST from <function distcalc at 0x0000028E1C9389D0> to <function distcalc at 0x0000028E1C9389D0>\n",
      "Attempt to reimplement DOC from <bound method ScreenIO.show_cmd_doc of <bluesky.simulation.screenio.ScreenIO object at 0x0000028E20FF10F0>> to <bound method ScreenIO.show_cmd_doc of <bluesky.simulation.screenio.ScreenIO object at 0x0000028E20FF10F0>>\n",
      "Attempt to reimplement DT from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099900> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94670>\n",
      "Attempt to reimplement DTMULT from <bound method Simulation.set_dtmult of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.set_dtmult of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement ECHO from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099990> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94A60>\n",
      "Attempt to reimplement FF from <bound method Simulation.fastforward of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.fastforward of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement FIXDT from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099A20> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94AF0>\n",
      "Attempt to reimplement GROUP from <bound method TrafficGroups.group of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x0000028E20FCAA10>> to <bound method TrafficGroups.group of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x0000028E20FCAA10>>\n",
      "Attempt to reimplement HOLD from <bound method Simulation.hold of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.hold of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement IMPLEMENTATION from <function select_implementation at 0x0000028E1C889240> to <function select_implementation at 0x0000028E1C889240>\n",
      "Attempt to reimplement INSEDIT from <bound method ScreenIO.cmdline of <bluesky.simulation.screenio.ScreenIO object at 0x0000028E20FF10F0>> to <bound method ScreenIO.cmdline of <bluesky.simulation.screenio.ScreenIO object at 0x0000028E20FF10F0>>\n",
      "Attempt to reimplement LEGEND from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099AB0> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94CA0>\n",
      "Attempt to reimplement LINE from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099B40> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94C10>\n",
      "Attempt to reimplement LSVAR from <function lsvar at 0x0000028E1C8B8D30> to <function lsvar at 0x0000028E1C8B8D30>\n",
      "Attempt to reimplement MAGVAR from <function magdeccmd at 0x0000028E1C8E2B90> to <function magdeccmd at 0x0000028E1C8E2B90>\n",
      "Attempt to reimplement MCRE from <bound method Traffic.mcre of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.mcre of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement MOVE from <bound method Traffic.move of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.move of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement NOISE from <bound method Traffic.setnoise of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.setnoise of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement OP from <bound method Simulation.op of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.op of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement PLOT from <function plot at 0x0000028E1C913400> to <function plot at 0x0000028E1C913400>\n",
      "Attempt to reimplement POLY from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099BD0> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E94310>\n",
      "Attempt to reimplement POLYALT from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099C60> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E95000>\n",
      "Attempt to reimplement POLYLINE from <function initbasecmds.<locals>.<lambda> at 0x0000028E21099CF0> to <function initbasecmds.<locals>.<lambda> at 0x0000028E45E956C0>\n",
      "Attempt to reimplement POS from <bound method Traffic.poscommand of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.poscommand of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement REALTIME from <bound method Simulation.realtime of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.realtime of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement RESET from <bound method Simulation.reset of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.reset of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement SEED from <function Simulation.setseed at 0x0000028E1C93ACB0> to <function Simulation.setseed at 0x0000028E1C93ACB0>\n",
      "Attempt to reimplement THR from <bound method Traffic.setthrottle of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>> to <bound method Traffic.setthrottle of <bluesky.traffic.traffic.Traffic object at 0x0000028E1C975030>>\n",
      "Attempt to reimplement TIME from <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x0000028E1C9744F0>> to <bound method Simulation.setutc of <bluesky.simulation.simulation.Simulation object at 0x0000028E3AB1C160>>\n",
      "Attempt to reimplement TRAIL from <bound method Trails.setTrails of <bluesky.traffic.trails.Trails object at 0x0000028E1C9749A0>> to <bound method Trails.setTrails of <bluesky.traffic.trails.Trails object at 0x0000028E1C9749A0>>\n",
      "Attempt to reimplement UNGROUP from <bound method TrafficGroups.ungroup of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x0000028E20FCAA10>> to <bound method TrafficGroups.ungroup of <bluesky.traffic.trafficgroups.TrafficGroups object at 0x0000028E20FCAA10>>\n",
      "Total reward (Q final): -54.44555459166666\n",
      "Steps: 41\n"
     ]
    }
   ],
   "source": [
    "env = DescentEnv(render_mode='human')     \n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    state = get_state(obs)\n",
    "    action = optimal_policy(state, Q)\n",
    "    obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward (Q final): {total_reward}\")\n",
    "print(f\"Steps: {steps}\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
