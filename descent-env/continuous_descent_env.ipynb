{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "# env = DescentEnv(render_mode='human')\n",
    "env = DescentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos\n",
    "\n",
    "**Discretizacion actualizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALT_MIN = 2000\n",
    "ALT_MAX = 4000\n",
    "ALT_MEAN = 1500\n",
    "ALT_STD = 3000\n",
    "VZ_MEAN = 0\n",
    "VZ_STD = 5\n",
    "RWY_DIS_MEAN = 100\n",
    "RWY_DIS_STD = 200\n",
    "altitude_space = np.linspace(0, 1, 70)\n",
    "vertical_velocity_space = np.linspace(-10, 10, 70) \n",
    "target_altitude_space = np.linspace(0, 1, 70)\n",
    "runway_distance_space = np.linspace(0, 0.5, 70)\n",
    "altitude_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    alt = obs['altitude'][0]\n",
    "    vz = obs['vz'][0]\n",
    "    target_alt = obs['target_altitude'][0]\n",
    "    runway_dist = obs['runway_distance'][0]\n",
    "    alt_idx = np.clip(np.digitize(alt, altitude_space) - 1, 0, len(altitude_space) - 1)\n",
    "    vz_idx = np.clip(np.digitize(vz, vertical_velocity_space) - 1, 0, len(vertical_velocity_space) - 1)\n",
    "    target_alt_idx = np.clip(np.digitize(target_alt, target_altitude_space) - 1, 0, len(target_altitude_space) - 1)\n",
    "    runway_dist_idx = np.clip(np.digitize(runway_dist, runway_distance_space) - 1, 0, len(runway_distance_space) - 1)\n",
    "    return alt_idx, vz_idx, target_alt_idx, runway_dist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs) # Ejemplo de obs\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(np.linspace(-1, 1, 30))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_action():\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((len(altitude_space), len(vertical_velocity_space), len(target_altitude_space), len(runway_distance_space), len(actions)))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = get_sample_action()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "\n",
    "min_runway_distance = float('inf')\n",
    "max_runway_distance = float('-inf')\n",
    "\n",
    "for _ in range(1):\n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    action_idx = actions.index(action)\n",
    "    real_action = np.array([action])\n",
    "    obs, reward, done, _, _ = env.step(real_action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "    # Guardar min y max runway_distance\n",
    "    runway_distance = obs['runway_distance'][0]\n",
    "    if runway_distance < min_runway_distance:\n",
    "        min_runway_distance = runway_distance\n",
    "    if runway_distance > max_runway_distance:\n",
    "        max_runway_distance = runway_distance\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        done = False\n",
    "\n",
    "env.close()\n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)\n",
    "print('min_runway_distance:', min_runway_distance)\n",
    "print('max_runway_distance:', max_runway_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m optimal_policy(state, Q)\n\u001b[1;32m---> 32\u001b[0m next_obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m next_state \u001b[38;5;241m=\u001b[39m get_state(next_obs)\n\u001b[0;32m     34\u001b[0m action_idx \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mindex(action)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\descent_env.py:185\u001b[0m, in \u001b[0;36mDescentEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    183\u001b[0m action_frequency \u001b[38;5;241m=\u001b[39m ACTION_FREQUENCY\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_frequency):\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame()\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\simulation\\simulation.py:107\u001b[0m, in \u001b[0;36mSimulation.step\u001b[1;34m(self, dt_increment)\u001b[0m\n\u001b[0;32m    105\u001b[0m plotter\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    106\u001b[0m datalog\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 107\u001b[0m \u001b[43mhooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Determine interval towards next timestep                \u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimdt \u001b[38;5;241m=\u001b[39m simtime\u001b[38;5;241m.\u001b[39mstep(dt_increment)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\timedfunction.py:13\u001b[0m, in \u001b[0;36m_Hook.trigger\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrigger\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 13\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\timedfunction.py:51\u001b[0m, in \u001b[0;36mtimed_function.<locals>.deco.<locals>.callback\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fobj)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcallback\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timer\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mfobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_act\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\core\\funcobject.py:32\u001b[0m, in \u001b[0;36mFuncObject.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\performance\\openap\\perfoap.py:249\u001b[0m, in \u001b[0;36mOpenAP.update\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuelflow[idx_fixwing] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengnum[idx_fixwing] \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_coeff_a[idx_fixwing] \u001b[38;5;241m*\u001b[39m thrustratio_fixwing \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_coeff_b[idx_fixwing] \u001b[38;5;241m*\u001b[39m thrustratio_fixwing\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_coeff_c[idx_fixwing]\n\u001b[0;32m    246\u001b[0m )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# ----- update max acceleration ----\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_axmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# TODO: implement thrust computation for rotor aircraft\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# idx_rotor = np.where(self.lifttype==coeff.LIFT_ROTOR)[0]\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# self.thrust[idx_rotor] = 0\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# update bank angle, due to phase change\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbank \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase \u001b[38;5;241m==\u001b[39m ph\u001b[38;5;241m.\u001b[39mGD), \u001b[38;5;241m15\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbank)\n",
      "File \u001b[1;32mc:\\idsem7\\Obligatorio IA - Marzo 2025\\descent-env\\.venv\\lib\\site-packages\\bluesky\\traffic\\performance\\openap\\perfoap.py:397\u001b[0m, in \u001b[0;36mOpenAP.calc_axmax\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m vmin, vmax\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmin[mask], vmax[mask]\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalc_axmax\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# accelerations depending on phase and wing type\u001b[39;00m\n\u001b[0;32m    399\u001b[0m     axmax_fixwing_ground \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    400\u001b[0m     axmax_rotor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.5\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# import sys\n",
    "# sys.stdout = open('output.txt', 'w')\n",
    "\n",
    "i = 0\n",
    "total_reward = 0\n",
    "rewards = []\n",
    "max_steps = 1\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "def get_explore_prob(i):\n",
    "    initial_epsilon = 0.9\n",
    "    min_epsilon = 0.05\n",
    "    decay_steps = 3530  # Redondeado hacia arriba\n",
    "    epsilon = max(initial_epsilon - (i // decay_steps) * 0.1, min_epsilon)\n",
    "    return epsilon\n",
    "\n",
    "while True:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action = optimal_policy(state, Q)\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        action_idx = actions.index(action)\n",
    "        Q[state][action_idx] = Q[state][action_idx] + 0.9 * (reward + 0.9 * np.max(Q[next_state]) - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    print(f\"Episode {i+1}, Reward: {episode_reward}, Epsilon: {prob}\")\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "        with open('Q.pkl', 'wb') as f:\n",
    "            pickle.dump(Q, f)\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Supongamos que Q es tu diccionario o matriz Q\n",
    "# Q = {...}\n",
    "\n",
    "# Guardar Q en un archivo .pkl\n",
    "with open('Q.pkl', 'wb') as f:\n",
    "    pickle.dump(Q, f)\n",
    "    \n",
    "import pprint\n",
    "\n",
    "pprint.pprint(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DescentEnv(render_mode='human')     \n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    state = get_state(obs)\n",
    "    action = optimal_policy(state, Q)\n",
    "    obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward (Q final): {total_reward}\")\n",
    "print(f\"Steps: {steps}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def stoch_argmax(Q_values, k=None):\n",
    "    n = len(Q_values)\n",
    "    if k is None:\n",
    "        k = max(1, int(math.log2(n)))  # O(log(n))\n",
    "    subset = random.sample(range(n), k)\n",
    "    best_action = subset[0]\n",
    "    best_value = Q_values[best_action]\n",
    "    for i in subset[1:]:\n",
    "        if Q_values[i] > best_value:\n",
    "            best_action = i\n",
    "            best_value = Q_values[i]\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_stochmax = 3  \n",
    "rewards = []\n",
    "i=0\n",
    "def get_explore_prob(i):\n",
    "    \"\"\"\n",
    "    Devuelve la probabilidad de exploración (epsilon) según el número de episodio i.\n",
    "    Alterna entre 0.7 y 0.3 cada 500 episodios.\n",
    "    \"\"\"\n",
    "    if 0 <= i <= 500:\n",
    "        return 0.7\n",
    "    elif 501 <= i <= 1000:\n",
    "        return 0.3\n",
    "    elif 1001 <= i <= 1500:\n",
    "        return 0.7\n",
    "    elif 1501 <= i <= 2000:\n",
    "        return 0.3\n",
    "    elif 2001 <= i <= 2500:\n",
    "        return 0.7\n",
    "    elif 2501 <= i <= 3000:\n",
    "        return 0.3\n",
    "    elif 3001 <= i <= 3500:\n",
    "        return 0.7\n",
    "    elif 3501 <= i <= 4000:\n",
    "        return 0.3\n",
    "    elif 4001 <= i <= 4500:\n",
    "        return 0.7\n",
    "    elif 4501 <= i <= 5000:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.1  # valor por defecto fuera de rango\n",
    "\n",
    "while i < 5000:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        p = random.uniform(0, 1)\n",
    "        state = get_state(obs)\n",
    "        prob = get_explore_prob(i)\n",
    "        if p < prob:\n",
    "            action = get_sample_action()\n",
    "        else:\n",
    "            action_idx = stoch_argmax(Q[state], k=k_stochmax)\n",
    "            action = actions[action_idx]\n",
    "        next_obs, reward, done, _, _ = env.step(np.array([action]))\n",
    "        next_state = get_state(next_obs)\n",
    "        best_next_action_idx = stoch_argmax(Q[next_state], k=k_stochmax)\n",
    "        Q[state][action_idx] += 0.9 * (reward + 0.9 * Q[next_state][best_next_action_idx] - Q[state][action_idx])\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Media de reward en episodios {i-98} a {i+1}: {mean_reward} con exploración {prob}\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "descent-env-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
